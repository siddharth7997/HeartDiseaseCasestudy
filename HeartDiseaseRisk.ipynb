{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Risk Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Used: UCI Heart Disease Dataset\n",
    "This directory contains 4 databases concerning heart disease diagnosis.\n",
    "   All attributes are numeric-valued.  The data was collected from the\n",
    "   four following locations:\n",
    "\n",
    "     1. Cleveland Clinic Foundation\n",
    "     2. Hungarian Institute of Cardiology, Budapest\n",
    "     3. V.A. Medical Center, Long Beach, CA\n",
    "     4. University Hospital, Zurich, Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Instances: \n",
    "####        Database:    # of instances:\n",
    "          1. Cleveland: 303\n",
    "          2. Hungarian: 294\n",
    "          3. Switzerland: 123\n",
    "          4. Long Beach VA: 200\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Information:\n",
    "      1. age:age in years       \n",
    "      2. sex:(1 = male; 0 = female)       \n",
    "      3. cp:chest pain type\n",
    "          -- Value 1: typical angina\n",
    "          -- Value 2: atypical angina\n",
    "          -- Value 3: non-anginal pain\n",
    "          -- Value 4: asymptomatic\n",
    "      4. trestbps: resting blood pressure  \n",
    "      5. chol:cholestoral      \n",
    "      6. fbs:(fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)    \n",
    "      7. restecg:\n",
    "          -- Value 0: normal\n",
    "          -- Value 1: having ST-T wave abnormality \n",
    "          -- Value 2: showing probable or definite left ventricular hypertrophy\n",
    "      8. thalach:maximum heart rate achieved\n",
    "      9. exang:exercise induced angina (1 = yes; 0 = no)     \n",
    "      10. oldpeak:ST depression induced by exercise relative to rest   \n",
    "      11. slope:the slope of the peak exercise ST segment\n",
    "        -- Value 1: upsloping\n",
    "        -- Value 2: flat\n",
    "        -- Value 3: downsloping     \n",
    "      12. ca: number of major vessels (0-3) colored by flourosopy        \n",
    "      13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "      14. category:diagnosis of heart disease[0-4]       (the predicted attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Information:\n",
    "     This database contains 76 attributes, but all published experiments\n",
    "     refer to using a subset of 14 of them.  In particular, the Cleveland\n",
    "     database is the only one that has been used by ML researchers to \n",
    "     this date.  The \"goal\" field refers to the presence of heart disease\n",
    "     in the patient.  It is integer valued from 0 (no presence) to 4.\n",
    "     Experiments with the Cleveland database have concentrated on simply\n",
    "     attempting to distinguish presence (values 1,2,3,4) from absence (value\n",
    "     0).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution:\n",
    "          Database:    0   1   2   3   4 Total\n",
    "          Cleveland: 164  55  36  35  13   303\n",
    "          Hungarian: 188  37  26  28  15   294\n",
    "        Switzerland:   8  48  32  30   5   123\n",
    "      Long Beach VA:  51  56  41  42  10   200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AGE  SEX  CP THRESTBPS CHOL FBS RESTECG THALACH EXANG OLDPEAK SLOPE CA  \\\n",
      "0   63    1   1       145  233   1       2     150     0     2.3     3  0   \n",
      "1   67    1   4       160  286   0       2     108     1     1.5     2  3   \n",
      "2   67    1   4       120  229   0       2     129     1     2.6     2  2   \n",
      "3   37    1   3       130  250   0       0     187     0     3.5     3  0   \n",
      "4   41    0   2       130  204   0       2     172     0     1.4     1  0   \n",
      "\n",
      "  THAL  CATEGORY  \n",
      "0    6         0  \n",
      "1    3         2  \n",
      "2    7         1  \n",
      "3    3         0  \n",
      "4    3         0  \n"
     ]
    }
   ],
   "source": [
    "df=pandas.read_csv('Preprocessed/data_combined.csv')\n",
    "print df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE           int64\n",
      "SEX           int64\n",
      "CP            int64\n",
      "THRESTBPS    object\n",
      "CHOL         object\n",
      "FBS          object\n",
      "RESTECG      object\n",
      "THALACH      object\n",
      "EXANG        object\n",
      "OLDPEAK      object\n",
      "SLOPE        object\n",
      "CA           object\n",
      "THAL         object\n",
      "CATEGORY      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    411\n",
      "1    265\n",
      "2    109\n",
      "3    107\n",
      "4     28\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Missing Attribute Values(WEKA TOOL)\n",
    "1. THRESTBPS(6%)\n",
    "2. RESTECG(2 values)\n",
    "2. CHOL(3%)\n",
    "3. FBS(10%)\n",
    "4. THALAC(6%)\n",
    "5. EXANG(6%)\n",
    "5. OLDPEAK(7%)\n",
    "6. SLOPE(34%)\n",
    "7. CA(66%)\n",
    "8. THAL(53%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values for THERESTBPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120    131\n",
      "130    115\n",
      "140    102\n",
      "110     59\n",
      "?       59\n",
      "Name: THRESTBPS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['THRESTBPS'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average rest blood pressure is  generally in range 120-140\n",
    "df['THRESTBPS'] = df['THRESTBPS'].replace(['?'],'120')\n",
    "df['THRESTBPS'] = df['THRESTBPS'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values for FBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    692\n",
      "1    138\n",
      "?     90\n",
      "Name: FBS, dtype: int64\n",
      "male:\n",
      "0    528\n",
      "1    119\n",
      "?     79\n",
      "Name: FBS, dtype: int64\n",
      "Female:\n",
      "0    164\n",
      "1     19\n",
      "?     11\n",
      "Name: FBS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print df.columns\n",
    "print df['FBS'].value_counts()\n",
    "print \"male:\\n\",df[df['SEX']==1]['FBS'].value_counts()\n",
    "print \"Female:\\n\",df[df['SEX']==0]['FBS'].value_counts()#directly replace with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    768\n",
      "1    152\n",
      "Name: FBS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#randomly filling values with 80% with 0 and 20% with 1s\n",
    "v=df.FBS.values=='?'\n",
    "df.loc[v, 'FBS'] = numpy.random.choice(('0','1'), v.sum(), p=(0.8,0.2))\n",
    "print df['FBS'].value_counts()\n",
    "df['FBS']=df['FBS'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing missing values in CHOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      172\n",
       "?       30\n",
       "254     10\n",
       "220     10\n",
       "216      9\n",
       "Name: CHOL, dtype: int64"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CHOL'].value_counts().head()\n",
    "#evenly distributed...\n",
    "#so will replace with mean of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['CHOL']=df['CHOL'].replace('?','-69')#temporarily replacing ? with -69\n",
    "df['CHOL']=df['CHOL'].astype('int64')\n",
    "k=int(df[df['CHOL']!=-69]['CHOL'].mean())\n",
    "df['CHOL']=df['CHOL'].replace(-69,k)\n",
    "\n",
    "\n",
    "#print df['CHOL'].unique() #completed !--!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values in RESTECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    551\n",
      "2    188\n",
      "1    179\n",
      "?      2\n",
      "Name: RESTECG, dtype: int64\n",
      "after replacing\n",
      "0    553\n",
      "2    188\n",
      "1    179\n",
      "Name: RESTECG, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['RESTECG'].value_counts()\n",
    "\n",
    "#replacing with max occuring value for attribute\n",
    "df['RESTECG']=df['RESTECG'].replace('?','0')\n",
    "#print df['RESTECG'].unique()\n",
    "#print df['RESTECG'].value_counts()\n",
    "df['RESTECG'] = df['RESTECG'].astype('int64')\n",
    "\n",
    "\n",
    "\n",
    "print \"after replacing\\n\",df['RESTECG'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values in THALACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "?      55\n",
       "150    43\n",
       "140    41\n",
       "120    35\n",
       "130    30\n",
       "Name: THALACH, dtype: int64"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['THALACH'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "df['THALACH']=df['THALACH'].replace('?','-69')#temporarily replacing ? with -69\n",
    "df['THALACH']=df['THALACH'].astype('int64')\n",
    "k=int(df[df['THALACH']!=-69]['THALACH'].mean())\n",
    "print k\n",
    "df['THALACH']=df['THALACH'].replace(-69,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137    60\n",
       "150    43\n",
       "140    41\n",
       "120    35\n",
       "130    30\n",
       "Name: THALACH, dtype: int64"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['THALACH'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values in EXANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    528\n",
      "1    337\n",
      "?     55\n",
      "Name: EXANG, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#exang:exercise induced angina (1 = yes; 0 = no) \n",
    "print df['EXANG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610404624277\n"
     ]
    }
   ],
   "source": [
    "k=528.0/(337.0+528.0)\n",
    "print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    565\n",
      "1    355\n",
      "Name: EXANG, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "v=df.EXANG.values=='?'\n",
    "df.loc[v,'EXANG'] = numpy.random.choice(('0','1'), v.sum(), p=(0.61,0.39))\n",
    "print df['EXANG'].value_counts()\n",
    "df['EXANG']=df[\"EXANG\"].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Replacing missing values in OLDPEAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      370\n",
      "1       83\n",
      "2       76\n",
      "?       62\n",
      "1.5     48\n",
      "Name: OLDPEAK, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['OLDPEAK'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878787878788\n"
     ]
    }
   ],
   "source": [
    "df['OLDPEAK']=df['OLDPEAK'].replace('?','-69')#temporarily replacing ? with -69\n",
    "df['OLDPEAK']=df['OLDPEAK'].astype('float64')\n",
    "k=df[df['OLDPEAK']!=-69]['OLDPEAK'].mean()\n",
    "print k\n",
    "df['OLDPEAK']=df['OLDPEAK'].replace(-69,numpy.round(k,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    370\n",
      "1.0     83\n",
      "2.0     76\n",
      "0.9     66\n",
      "1.5     48\n",
      "Name: OLDPEAK, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['OLDPEAK'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLOPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    345\n",
      "?    309\n",
      "1    203\n",
      "3     63\n",
      "Name: SLOPE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['SLOPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k=203.0/(345.0+203.0+63.0)\n",
    "#print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    525\n",
      "1    299\n",
      "3     96\n",
      "Name: SLOPE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "v=df.SLOPE.values=='?'\n",
    "df.loc[v,'SLOPE'] = numpy.random.choice(('2','1','3'), v.sum(), p=(0.6,0.30,0.10))\n",
    "print df['SLOPE'].value_counts()\n",
    "df['SLOPE']=df['SLOPE'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?    611\n",
      "0    181\n",
      "1     67\n",
      "2     41\n",
      "3     20\n",
      "Name: CA, dtype: int64\n",
      "0.132686084142\n"
     ]
    }
   ],
   "source": [
    "print df[\"CA\"].value_counts()\n",
    "k=(41.0)/(181+67+41+20)\n",
    "print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    568\n",
      "1    179\n",
      "2    112\n",
      "3     61\n",
      "Name: CA, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "v=df.CA.values=='?'\n",
    "df.loc[v,'CA'] = numpy.random.choice(('0','1','2','3'), v.sum(), p=(0.60,0.20,0.13,0.07))\n",
    "df['CA']=df['CA'].astype('int64')\n",
    "print df['CA'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?    486\n",
      "3    196\n",
      "7    192\n",
      "6     46\n",
      "Name: THAL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['THAL'].value_counts()\n",
    "#can't use random walk directly here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    110\n",
      "0     86\n",
      "Name: SEX, dtype: int64\n",
      "1    171\n",
      "0     21\n",
      "Name: SEX, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df[df['THAL']=='3']['SEX'].value_counts()\n",
    "print df[df['THAL']=='7']['SEX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THAL:3=====>\n",
      "0    138\n",
      "1     30\n",
      "2     14\n",
      "3     12\n",
      "4      2\n",
      "Name: CATEGORY, dtype: int64\n",
      "THAL:7=====>\n",
      "1    63\n",
      "3    43\n",
      "0    38\n",
      "2    37\n",
      "4    11\n",
      "Name: CATEGORY, dtype: int64\n",
      "THAL:6=====>\n",
      "1    13\n",
      "2    12\n",
      "0    11\n",
      "3     7\n",
      "4     3\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print \"THAL:3=====>\\n\",df[df['THAL']=='3']['CATEGORY'].value_counts()\n",
    "print \"THAL:7=====>\\n\",df[df['THAL']=='7']['CATEGORY'].value_counts()\n",
    "print \"THAL:6=====>\\n\",df[df['THAL']=='6']['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    454\n",
      "3    420\n",
      "6     46\n",
      "Name: THAL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['THAL']=df['THAL'].replace('?',-1)\n",
    "'''\n",
    "df['THAL']=df['THAL'].replace('?',-1)\n",
    "for row in df.iterrows():\n",
    "    if row['THAL']==-1 and row['CATEGORY']>=1:\n",
    "        df.loc[row.Index, 'ifor'] = 7\n",
    "        \n",
    "    elif row['THAL']==-1 and row['CATEGORY']==0:\n",
    "        df.loc[row.Index, 'ifor'] = 3\n",
    "'''\n",
    "df.loc[(df['THAL']==-1)&(df['CATEGORY']!=0),'THAL']='7'\n",
    "#print df['THAL'].value_counts()\n",
    "df.loc[(df['THAL']==-1)&(df['CATEGORY']==0),'THAL']='3'\n",
    "print df['THAL'].value_counts()\n",
    "df['THAL']=df['THAL'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE            int64\n",
      "SEX            int64\n",
      "CP             int64\n",
      "THRESTBPS      int64\n",
      "CHOL           int64\n",
      "FBS            int64\n",
      "RESTECG        int64\n",
      "THALACH        int64\n",
      "EXANG          int64\n",
      "OLDPEAK      float64\n",
      "SLOPE          int64\n",
      "CA             int64\n",
      "THAL           int64\n",
      "CATEGORY       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummies = pandas.get_dummies(df[\"CP\"],prefix=\"CP\")\n",
    "df = df.join(dummies)\n",
    "\n",
    "dummies = pandas.get_dummies(df[\"RESTECG\"],prefix=\"RESTECG\")\n",
    "df      = df.join(dummies)\n",
    "\n",
    "dummies = pandas.get_dummies(df[\"SLOPE\"],prefix=\"SLOPE\")\n",
    "df      = df.join(dummies)\n",
    "\n",
    "dummies = pandas.get_dummies(df[\"THAL\"],prefix=\"THAL\")\n",
    "df      = df.join(dummies)\n",
    "\n",
    "#dummies = pandas.get_dummies(df[\"EXANG\"],prefix=\"EXANG\")\n",
    "#df = df.join(dummies)\n",
    "\n",
    "#del df['SEX']\n",
    "del df['CP']\n",
    "del df['RESTECG']\n",
    "del df['SLOPE']\n",
    "del df['THAL']\n",
    "#del df['EXANG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE            int64\n",
      "SEX            int64\n",
      "THRESTBPS      int64\n",
      "CHOL           int64\n",
      "FBS            int64\n",
      "THALACH        int64\n",
      "EXANG          int64\n",
      "OLDPEAK      float64\n",
      "CA             int64\n",
      "CATEGORY       int64\n",
      "CP_1           uint8\n",
      "CP_2           uint8\n",
      "CP_3           uint8\n",
      "CP_4           uint8\n",
      "RESTECG_0      uint8\n",
      "RESTECG_1      uint8\n",
      "RESTECG_2      uint8\n",
      "SLOPE_1        uint8\n",
      "SLOPE_2        uint8\n",
      "SLOPE_3        uint8\n",
      "THAL_3         uint8\n",
      "THAL_6         uint8\n",
      "THAL_7         uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for g in df.columns:\n",
    "    if df[g].dtype=='uint8':\n",
    "        df[g]=df[g].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df.loc[df['CATEGORY']>0,'CATEGORY']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE 2.49229867231 -2.70681396757\n",
      "THRESTBPS 3.67440591791 -7.03102349108\n",
      "CHOL 3.70670590542 -1.82755513195\n",
      "THALACH 2.56523324659 -3.08339929343\n",
      "OLDPEAK 5.04825042025 -3.30258023693\n",
      "CA 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "stdcols = [\"AGE\",\"THRESTBPS\",\"CHOL\",\"THALACH\",\"OLDPEAK\"]\n",
    "nrmcols = [\"CA\"]\n",
    "stddf   = df.copy()\n",
    "stddf[stdcols] = stddf[stdcols].apply(lambda x: (x-x.mean())/x.std())\n",
    "stddf[nrmcols] = stddf[nrmcols].apply(lambda x: (x-x.min())/(x.max()-x.min()))\n",
    "#stddf[stdcols] = stddf[stdcols].apply(lambda x: (x-x.mean())/(x.max()-x.min()))\n",
    "\n",
    "\n",
    "for g in stdcols:\n",
    "    print g,max(stddf[g]),min(stddf[g])\n",
    "    \n",
    "for g in nrmcols:\n",
    "    print g,max(stddf[g]),min(stddf[g])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE          float64\n",
      "SEX            int64\n",
      "THRESTBPS    float64\n",
      "CHOL         float64\n",
      "FBS            int64\n",
      "THALACH      float64\n",
      "EXANG          int64\n",
      "OLDPEAK      float64\n",
      "CA           float64\n",
      "CATEGORY       int64\n",
      "CP_1           int64\n",
      "CP_2           int64\n",
      "CP_3           int64\n",
      "CP_4           int64\n",
      "RESTECG_0      int64\n",
      "RESTECG_1      int64\n",
      "RESTECG_2      int64\n",
      "SLOPE_1        int64\n",
      "SLOPE_2        int64\n",
      "SLOPE_3        int64\n",
      "THAL_3         int64\n",
      "THAL_6         int64\n",
      "THAL_7         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print stddf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'> (920, 22)\n"
     ]
    }
   ],
   "source": [
    "df_copy=stddf.copy()\n",
    "df_copy=df_copy.drop(['CATEGORY'],axis=1)\n",
    "\n",
    "dat=df_copy.values\n",
    "#print dat.shape\n",
    "\n",
    "print type(dat),dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0] <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "labels=df['CATEGORY'].values\n",
    "print labels[:5],type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    509\n",
      "0    411\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(dat,labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (690, 22)\n",
      "y_train: (690,)\n",
      "\n",
      "x_test: (230, 22)\n",
      "y_test: (230,)\n"
     ]
    }
   ],
   "source": [
    "print \"x_train:\",x_train.shape\n",
    "print \"y_train:\",y_train.shape\n",
    "print\n",
    "print \"x_test:\",x_test.shape\n",
    "print \"y_test:\",y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 89.5652173913 %\n",
      "Logistic Regression: 88.6956521739 %\n"
     ]
    }
   ],
   "source": [
    "#training and testing\n",
    "#SVM\n",
    "from sklearn import svm\n",
    "clfsvm= svm.SVC(gamma=0.001, C=5)\n",
    "clfsvm.fit(x_train,y_train)\n",
    "print \"SVM:\",clfsvm.score(x_test,y_test)*100,\"%\"\n",
    "svmpred=clfsvm.predict(x_test)\n",
    "#print svmpred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#scores = cross_val_score(clf,dat,labels, cv=5)\n",
    "#print scores\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "lrcv=linear_model.LogisticRegressionCV(fit_intercept=True,penalty='l2',dual=False)\n",
    "lrcv.fit(x_train,y_train)\n",
    "print \"Logistic Regression:\",lrcv.score(x_test,y_test)*100,\"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of various features\n",
      "AGE 0.0425005214349\n",
      "SEX 0.0209324459398\n",
      "THRESTBPS 0.0365276112325\n",
      "CHOL 0.0629200171632\n",
      "FBS 0.0119640642529\n",
      "THALACH 0.0676923923224\n",
      "EXANG 0.0678499321429\n",
      "OLDPEAK 0.041210523927\n",
      "CA 0.0348150167151\n",
      "CP_1 0.00914255742067\n",
      "CP_2 0.0306477759846\n",
      "CP_3 0.00700040573836\n",
      "CP_4 0.103558953636\n",
      "RESTECG_0 0.0127939079927\n",
      "RESTECG_1 0.014416644744\n",
      "RESTECG_2 0.0123279676384\n",
      "SLOPE_1 0.0223094639076\n",
      "SLOPE_2 0.00730892616285\n",
      "SLOPE_3 0.00573128023\n",
      "THAL_3 0.253168772905\n",
      "THAL_6 0.0161352272832\n",
      "THAL_7 0.119045591225\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAKJCAYAAADgPCs8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb57b9e7810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after feature sel SVM: 90.0 %\n",
      "Logistic Regression: 90.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(dat,labels)\n",
    "g=clf.feature_importances_\n",
    "c=stddf.drop(['CATEGORY'],axis=1).columns\n",
    "\n",
    "print \"Importance of various features\"\n",
    "for k in range(len(c)):\n",
    "    print c[k],g[k]\n",
    "\n",
    "g=g*100    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] =[15,10]\n",
    "\n",
    "index = numpy.arange(len(c))\n",
    "plt.bar(index,g)\n",
    "plt.xlabel('Attributes', fontsize=14)\n",
    "plt.ylabel('importance', fontsize=14)\n",
    "plt.xticks(index,c, fontsize=12, rotation=30)\n",
    "plt.title('Importance of various features',fontsize=18)\n",
    "plt.show()\n",
    "    \n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(dat)\n",
    "#print X_new.shape\n",
    "\n",
    "\n",
    "tx_train,tx_test,ty_train,ty_test=train_test_split(X_new,labels, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "tclf = svm.SVC(gamma=0.001, C=5)\n",
    "tclf.fit(tx_train,ty_train)\n",
    "print \"after feature sel SVM:\",tclf.score(tx_test,ty_test)*100,\"%\"\n",
    "tsvmpred=tclf.predict(tx_test)\n",
    "#print tsvmpred\n",
    "\n",
    "\n",
    "tlrcv=linear_model.LogisticRegressionCV(fit_intercept=True,penalty='l2',dual=False)\n",
    "tlrcv.fit(tx_train,ty_train)\n",
    "print \"Logistic Regression:\",tlrcv.score(tx_test,ty_test)*100,\"%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import tensorflow\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dropout, Flatten, Activation, Dense\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(activ,opti,ip,layers,trainx,trainy,testx,testy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0], input_dim=ip, init='uniform', activation=activ))\n",
    "    model.add(Dense(layers[1], init='uniform', activation=activ))\n",
    "    model.add(Dense(1, init='uniform', activation=activ))\n",
    "    model.compile(loss='mean_squared_logarithmic_error', optimizer=opti, metrics=['accuracy'])\n",
    "    model.fit(trainx,trainy,epochs=600,batch_size=512,verbose=2,validation_data=(testx,testy))\n",
    "    \n",
    "    trainScore = model.evaluate(trainx,trainy, verbose=0)\n",
    "    print \"Train Score: \",100-trainScore[0]*100\n",
    "    testScore = model.evaluate(testx,testy, verbose=0)\n",
    "    print \"Test Score: \",100-testScore[0]*100\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/miniconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(22, activation=\"sigmoid\", kernel_initializer=\"uniform\", input_dim=22)`\n",
      "  app.launch_new_instance()\n",
      "/home/siddharth/miniconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "/home/siddharth/miniconda2/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 690 samples, validate on 230 samples\n",
      "Epoch 1/600\n",
      "0s - loss: 0.1196 - acc: 0.4565 - val_loss: 0.1170 - val_acc: 0.4174\n",
      "Epoch 2/600\n",
      "0s - loss: 0.1194 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 3/600\n",
      "0s - loss: 0.1193 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 4/600\n",
      "0s - loss: 0.1193 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 5/600\n",
      "0s - loss: 0.1193 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 6/600\n",
      "0s - loss: 0.1192 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 7/600\n",
      "0s - loss: 0.1192 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 8/600\n",
      "0s - loss: 0.1192 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 9/600\n",
      "0s - loss: 0.1191 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 10/600\n",
      "0s - loss: 0.1191 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 11/600\n",
      "0s - loss: 0.1191 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 12/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 13/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 14/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1175 - val_acc: 0.4174\n",
      "Epoch 15/600\n",
      "0s - loss: 0.1189 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 16/600\n",
      "0s - loss: 0.1189 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 17/600\n",
      "0s - loss: 0.1189 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 18/600\n",
      "0s - loss: 0.1189 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 19/600\n",
      "0s - loss: 0.1188 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 20/600\n",
      "0s - loss: 0.1188 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 21/600\n",
      "0s - loss: 0.1187 - acc: 0.4565 - val_loss: 0.1170 - val_acc: 0.4174\n",
      "Epoch 22/600\n",
      "0s - loss: 0.1187 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 23/600\n",
      "0s - loss: 0.1186 - acc: 0.4565 - val_loss: 0.1170 - val_acc: 0.4174\n",
      "Epoch 24/600\n",
      "0s - loss: 0.1185 - acc: 0.4565 - val_loss: 0.1170 - val_acc: 0.4174\n",
      "Epoch 25/600\n",
      "0s - loss: 0.1184 - acc: 0.4565 - val_loss: 0.1169 - val_acc: 0.4174\n",
      "Epoch 26/600\n",
      "0s - loss: 0.1183 - acc: 0.4565 - val_loss: 0.1168 - val_acc: 0.4174\n",
      "Epoch 27/600\n",
      "0s - loss: 0.1182 - acc: 0.4565 - val_loss: 0.1167 - val_acc: 0.4174\n",
      "Epoch 28/600\n",
      "0s - loss: 0.1181 - acc: 0.4565 - val_loss: 0.1167 - val_acc: 0.4174\n",
      "Epoch 29/600\n",
      "0s - loss: 0.1180 - acc: 0.4565 - val_loss: 0.1166 - val_acc: 0.4174\n",
      "Epoch 30/600\n",
      "0s - loss: 0.1179 - acc: 0.4565 - val_loss: 0.1165 - val_acc: 0.4174\n",
      "Epoch 31/600\n",
      "0s - loss: 0.1177 - acc: 0.4565 - val_loss: 0.1163 - val_acc: 0.4174\n",
      "Epoch 32/600\n",
      "0s - loss: 0.1176 - acc: 0.4565 - val_loss: 0.1163 - val_acc: 0.4174\n",
      "Epoch 33/600\n",
      "0s - loss: 0.1174 - acc: 0.4565 - val_loss: 0.1161 - val_acc: 0.4174\n",
      "Epoch 34/600\n",
      "0s - loss: 0.1173 - acc: 0.4565 - val_loss: 0.1160 - val_acc: 0.4174\n",
      "Epoch 35/600\n",
      "0s - loss: 0.1171 - acc: 0.4565 - val_loss: 0.1157 - val_acc: 0.4174\n",
      "Epoch 36/600\n",
      "0s - loss: 0.1169 - acc: 0.4565 - val_loss: 0.1155 - val_acc: 0.4174\n",
      "Epoch 37/600\n",
      "0s - loss: 0.1167 - acc: 0.4565 - val_loss: 0.1153 - val_acc: 0.4174\n",
      "Epoch 38/600\n",
      "0s - loss: 0.1165 - acc: 0.4565 - val_loss: 0.1152 - val_acc: 0.4174\n",
      "Epoch 39/600\n",
      "0s - loss: 0.1163 - acc: 0.4565 - val_loss: 0.1150 - val_acc: 0.4174\n",
      "Epoch 40/600\n",
      "0s - loss: 0.1161 - acc: 0.4565 - val_loss: 0.1147 - val_acc: 0.4174\n",
      "Epoch 41/600\n",
      "0s - loss: 0.1159 - acc: 0.4565 - val_loss: 0.1145 - val_acc: 0.4174\n",
      "Epoch 42/600\n",
      "0s - loss: 0.1156 - acc: 0.4565 - val_loss: 0.1142 - val_acc: 0.4174\n",
      "Epoch 43/600\n",
      "0s - loss: 0.1154 - acc: 0.4565 - val_loss: 0.1141 - val_acc: 0.4174\n",
      "Epoch 44/600\n",
      "0s - loss: 0.1151 - acc: 0.4565 - val_loss: 0.1139 - val_acc: 0.4174\n",
      "Epoch 45/600\n",
      "0s - loss: 0.1148 - acc: 0.4565 - val_loss: 0.1136 - val_acc: 0.4174\n",
      "Epoch 46/600\n",
      "0s - loss: 0.1146 - acc: 0.4565 - val_loss: 0.1133 - val_acc: 0.4174\n",
      "Epoch 47/600\n",
      "0s - loss: 0.1143 - acc: 0.4565 - val_loss: 0.1131 - val_acc: 0.4174\n",
      "Epoch 48/600\n",
      "0s - loss: 0.1140 - acc: 0.4565 - val_loss: 0.1128 - val_acc: 0.4174\n",
      "Epoch 49/600\n",
      "0s - loss: 0.1136 - acc: 0.4565 - val_loss: 0.1125 - val_acc: 0.4174\n",
      "Epoch 50/600\n",
      "0s - loss: 0.1133 - acc: 0.4565 - val_loss: 0.1123 - val_acc: 0.4174\n",
      "Epoch 51/600\n",
      "0s - loss: 0.1129 - acc: 0.4565 - val_loss: 0.1120 - val_acc: 0.4174\n",
      "Epoch 52/600\n",
      "0s - loss: 0.1126 - acc: 0.4565 - val_loss: 0.1117 - val_acc: 0.4174\n",
      "Epoch 53/600\n",
      "0s - loss: 0.1122 - acc: 0.4565 - val_loss: 0.1113 - val_acc: 0.4174\n",
      "Epoch 54/600\n",
      "0s - loss: 0.1118 - acc: 0.4565 - val_loss: 0.1110 - val_acc: 0.4174\n",
      "Epoch 55/600\n",
      "0s - loss: 0.1114 - acc: 0.4565 - val_loss: 0.1106 - val_acc: 0.4174\n",
      "Epoch 56/600\n",
      "0s - loss: 0.1110 - acc: 0.4565 - val_loss: 0.1103 - val_acc: 0.4174\n",
      "Epoch 57/600\n",
      "0s - loss: 0.1106 - acc: 0.4565 - val_loss: 0.1099 - val_acc: 0.4174\n",
      "Epoch 58/600\n",
      "0s - loss: 0.1102 - acc: 0.4565 - val_loss: 0.1095 - val_acc: 0.4174\n",
      "Epoch 59/600\n",
      "0s - loss: 0.1097 - acc: 0.4565 - val_loss: 0.1090 - val_acc: 0.4174\n",
      "Epoch 60/600\n",
      "0s - loss: 0.1093 - acc: 0.4565 - val_loss: 0.1086 - val_acc: 0.4174\n",
      "Epoch 61/600\n",
      "0s - loss: 0.1088 - acc: 0.4565 - val_loss: 0.1081 - val_acc: 0.4174\n",
      "Epoch 62/600\n",
      "0s - loss: 0.1084 - acc: 0.4580 - val_loss: 0.1077 - val_acc: 0.4174\n",
      "Epoch 63/600\n",
      "0s - loss: 0.1080 - acc: 0.4609 - val_loss: 0.1073 - val_acc: 0.4217\n",
      "Epoch 64/600\n",
      "0s - loss: 0.1075 - acc: 0.4696 - val_loss: 0.1069 - val_acc: 0.4217\n",
      "Epoch 65/600\n",
      "0s - loss: 0.1070 - acc: 0.4696 - val_loss: 0.1064 - val_acc: 0.4391\n",
      "Epoch 66/600\n",
      "0s - loss: 0.1065 - acc: 0.4739 - val_loss: 0.1059 - val_acc: 0.4391\n",
      "Epoch 67/600\n",
      "0s - loss: 0.1060 - acc: 0.4754 - val_loss: 0.1054 - val_acc: 0.4652\n",
      "Epoch 68/600\n",
      "0s - loss: 0.1055 - acc: 0.4884 - val_loss: 0.1049 - val_acc: 0.4696\n",
      "Epoch 69/600\n",
      "0s - loss: 0.1051 - acc: 0.5014 - val_loss: 0.1044 - val_acc: 0.5130\n",
      "Epoch 70/600\n",
      "0s - loss: 0.1046 - acc: 0.5145 - val_loss: 0.1040 - val_acc: 0.5261\n",
      "Epoch 71/600\n",
      "0s - loss: 0.1041 - acc: 0.5348 - val_loss: 0.1035 - val_acc: 0.5304\n",
      "Epoch 72/600\n",
      "0s - loss: 0.1036 - acc: 0.5362 - val_loss: 0.1031 - val_acc: 0.5261\n",
      "Epoch 73/600\n",
      "0s - loss: 0.1030 - acc: 0.5449 - val_loss: 0.1026 - val_acc: 0.5261\n",
      "Epoch 74/600\n",
      "0s - loss: 0.1025 - acc: 0.5449 - val_loss: 0.1022 - val_acc: 0.5304\n",
      "Epoch 75/600\n",
      "0s - loss: 0.1019 - acc: 0.5493 - val_loss: 0.1016 - val_acc: 0.5391\n",
      "Epoch 76/600\n",
      "0s - loss: 0.1013 - acc: 0.5739 - val_loss: 0.1011 - val_acc: 0.5435\n",
      "Epoch 77/600\n",
      "0s - loss: 0.1008 - acc: 0.5855 - val_loss: 0.1006 - val_acc: 0.5522\n",
      "Epoch 78/600\n",
      "0s - loss: 0.1002 - acc: 0.5957 - val_loss: 0.1001 - val_acc: 0.5696\n",
      "Epoch 79/600\n",
      "0s - loss: 0.0996 - acc: 0.6014 - val_loss: 0.0995 - val_acc: 0.5826\n",
      "Epoch 80/600\n",
      "0s - loss: 0.0990 - acc: 0.6116 - val_loss: 0.0989 - val_acc: 0.5957\n",
      "Epoch 81/600\n",
      "0s - loss: 0.0984 - acc: 0.6217 - val_loss: 0.0984 - val_acc: 0.6000\n",
      "Epoch 82/600\n",
      "0s - loss: 0.0977 - acc: 0.6290 - val_loss: 0.0978 - val_acc: 0.6130\n",
      "Epoch 83/600\n",
      "0s - loss: 0.0971 - acc: 0.6333 - val_loss: 0.0972 - val_acc: 0.6217\n",
      "Epoch 84/600\n",
      "0s - loss: 0.0965 - acc: 0.6536 - val_loss: 0.0967 - val_acc: 0.6261\n",
      "Epoch 85/600\n",
      "0s - loss: 0.0959 - acc: 0.6536 - val_loss: 0.0960 - val_acc: 0.6304\n",
      "Epoch 86/600\n",
      "0s - loss: 0.0952 - acc: 0.6739 - val_loss: 0.0955 - val_acc: 0.6304\n",
      "Epoch 87/600\n",
      "0s - loss: 0.0946 - acc: 0.6725 - val_loss: 0.0949 - val_acc: 0.6304\n",
      "Epoch 88/600\n",
      "0s - loss: 0.0939 - acc: 0.6739 - val_loss: 0.0943 - val_acc: 0.6304\n",
      "Epoch 89/600\n",
      "0s - loss: 0.0933 - acc: 0.6768 - val_loss: 0.0937 - val_acc: 0.6478\n",
      "Epoch 90/600\n",
      "0s - loss: 0.0927 - acc: 0.6957 - val_loss: 0.0931 - val_acc: 0.6522\n",
      "Epoch 91/600\n",
      "0s - loss: 0.0921 - acc: 0.7000 - val_loss: 0.0925 - val_acc: 0.6652\n",
      "Epoch 92/600\n",
      "0s - loss: 0.0914 - acc: 0.7101 - val_loss: 0.0919 - val_acc: 0.6739\n",
      "Epoch 93/600\n",
      "0s - loss: 0.0909 - acc: 0.7145 - val_loss: 0.0912 - val_acc: 0.6913\n",
      "Epoch 94/600\n",
      "0s - loss: 0.0902 - acc: 0.7362 - val_loss: 0.0907 - val_acc: 0.6957\n",
      "Epoch 95/600\n",
      "0s - loss: 0.0896 - acc: 0.7377 - val_loss: 0.0901 - val_acc: 0.7000\n",
      "Epoch 96/600\n",
      "0s - loss: 0.0890 - acc: 0.7435 - val_loss: 0.0895 - val_acc: 0.7000\n",
      "Epoch 97/600\n",
      "0s - loss: 0.0884 - acc: 0.7464 - val_loss: 0.0889 - val_acc: 0.7130\n",
      "Epoch 98/600\n",
      "0s - loss: 0.0878 - acc: 0.7507 - val_loss: 0.0884 - val_acc: 0.7130\n",
      "Epoch 99/600\n",
      "0s - loss: 0.0872 - acc: 0.7522 - val_loss: 0.0879 - val_acc: 0.7130\n",
      "Epoch 100/600\n",
      "0s - loss: 0.0865 - acc: 0.7551 - val_loss: 0.0872 - val_acc: 0.7174\n",
      "Epoch 101/600\n",
      "0s - loss: 0.0859 - acc: 0.7594 - val_loss: 0.0866 - val_acc: 0.7261\n",
      "Epoch 102/600\n",
      "0s - loss: 0.0853 - acc: 0.7710 - val_loss: 0.0861 - val_acc: 0.7261\n",
      "Epoch 103/600\n",
      "0s - loss: 0.0847 - acc: 0.7725 - val_loss: 0.0854 - val_acc: 0.7348\n",
      "Epoch 104/600\n",
      "0s - loss: 0.0841 - acc: 0.7812 - val_loss: 0.0849 - val_acc: 0.7348\n",
      "Epoch 105/600\n",
      "0s - loss: 0.0835 - acc: 0.7870 - val_loss: 0.0843 - val_acc: 0.7348\n",
      "Epoch 106/600\n",
      "0s - loss: 0.0828 - acc: 0.7870 - val_loss: 0.0838 - val_acc: 0.7348\n",
      "Epoch 107/600\n",
      "0s - loss: 0.0822 - acc: 0.7870 - val_loss: 0.0831 - val_acc: 0.7304\n",
      "Epoch 108/600\n",
      "0s - loss: 0.0816 - acc: 0.7942 - val_loss: 0.0825 - val_acc: 0.7391\n",
      "Epoch 109/600\n",
      "0s - loss: 0.0810 - acc: 0.7942 - val_loss: 0.0819 - val_acc: 0.7435\n",
      "Epoch 110/600\n",
      "0s - loss: 0.0804 - acc: 0.7957 - val_loss: 0.0813 - val_acc: 0.7522\n",
      "Epoch 111/600\n",
      "0s - loss: 0.0798 - acc: 0.7971 - val_loss: 0.0808 - val_acc: 0.7522\n",
      "Epoch 112/600\n",
      "0s - loss: 0.0792 - acc: 0.7986 - val_loss: 0.0801 - val_acc: 0.7522\n",
      "Epoch 113/600\n",
      "0s - loss: 0.0786 - acc: 0.8014 - val_loss: 0.0796 - val_acc: 0.7565\n",
      "Epoch 114/600\n",
      "0s - loss: 0.0780 - acc: 0.8072 - val_loss: 0.0790 - val_acc: 0.7739\n",
      "Epoch 115/600\n",
      "0s - loss: 0.0774 - acc: 0.8101 - val_loss: 0.0784 - val_acc: 0.7739\n",
      "Epoch 116/600\n",
      "0s - loss: 0.0768 - acc: 0.8101 - val_loss: 0.0778 - val_acc: 0.7826\n",
      "Epoch 117/600\n",
      "0s - loss: 0.0762 - acc: 0.8116 - val_loss: 0.0773 - val_acc: 0.7826\n",
      "Epoch 118/600\n",
      "0s - loss: 0.0756 - acc: 0.8130 - val_loss: 0.0768 - val_acc: 0.7783\n",
      "Epoch 119/600\n",
      "0s - loss: 0.0751 - acc: 0.8116 - val_loss: 0.0763 - val_acc: 0.7826\n",
      "Epoch 120/600\n",
      "0s - loss: 0.0745 - acc: 0.8174 - val_loss: 0.0757 - val_acc: 0.7826\n",
      "Epoch 121/600\n",
      "0s - loss: 0.0739 - acc: 0.8159 - val_loss: 0.0753 - val_acc: 0.7826\n",
      "Epoch 122/600\n",
      "0s - loss: 0.0734 - acc: 0.8174 - val_loss: 0.0748 - val_acc: 0.7870\n",
      "Epoch 123/600\n",
      "0s - loss: 0.0728 - acc: 0.8217 - val_loss: 0.0742 - val_acc: 0.7870\n",
      "Epoch 124/600\n",
      "0s - loss: 0.0723 - acc: 0.8261 - val_loss: 0.0737 - val_acc: 0.7870\n",
      "Epoch 125/600\n",
      "0s - loss: 0.0717 - acc: 0.8290 - val_loss: 0.0733 - val_acc: 0.7870\n",
      "Epoch 126/600\n",
      "0s - loss: 0.0712 - acc: 0.8304 - val_loss: 0.0727 - val_acc: 0.7826\n",
      "Epoch 127/600\n",
      "0s - loss: 0.0707 - acc: 0.8333 - val_loss: 0.0721 - val_acc: 0.7913\n",
      "Epoch 128/600\n",
      "0s - loss: 0.0702 - acc: 0.8319 - val_loss: 0.0715 - val_acc: 0.7957\n",
      "Epoch 129/600\n",
      "0s - loss: 0.0696 - acc: 0.8290 - val_loss: 0.0710 - val_acc: 0.8000\n",
      "Epoch 130/600\n",
      "0s - loss: 0.0691 - acc: 0.8319 - val_loss: 0.0707 - val_acc: 0.7957\n",
      "Epoch 131/600\n",
      "0s - loss: 0.0686 - acc: 0.8319 - val_loss: 0.0702 - val_acc: 0.7957\n",
      "Epoch 132/600\n",
      "0s - loss: 0.0681 - acc: 0.8333 - val_loss: 0.0698 - val_acc: 0.7957\n",
      "Epoch 133/600\n",
      "0s - loss: 0.0676 - acc: 0.8377 - val_loss: 0.0693 - val_acc: 0.7957\n",
      "Epoch 134/600\n",
      "0s - loss: 0.0671 - acc: 0.8377 - val_loss: 0.0688 - val_acc: 0.7957\n",
      "Epoch 135/600\n",
      "0s - loss: 0.0666 - acc: 0.8391 - val_loss: 0.0684 - val_acc: 0.8000\n",
      "Epoch 136/600\n",
      "0s - loss: 0.0661 - acc: 0.8406 - val_loss: 0.0679 - val_acc: 0.8000\n",
      "Epoch 137/600\n",
      "0s - loss: 0.0656 - acc: 0.8406 - val_loss: 0.0675 - val_acc: 0.8043\n",
      "Epoch 138/600\n",
      "0s - loss: 0.0652 - acc: 0.8435 - val_loss: 0.0670 - val_acc: 0.8130\n",
      "Epoch 139/600\n",
      "0s - loss: 0.0647 - acc: 0.8478 - val_loss: 0.0665 - val_acc: 0.8174\n",
      "Epoch 140/600\n",
      "0s - loss: 0.0642 - acc: 0.8493 - val_loss: 0.0660 - val_acc: 0.8217\n",
      "Epoch 141/600\n",
      "0s - loss: 0.0637 - acc: 0.8507 - val_loss: 0.0655 - val_acc: 0.8174\n",
      "Epoch 142/600\n",
      "0s - loss: 0.0633 - acc: 0.8551 - val_loss: 0.0652 - val_acc: 0.8174\n",
      "Epoch 143/600\n",
      "0s - loss: 0.0628 - acc: 0.8551 - val_loss: 0.0648 - val_acc: 0.8174\n",
      "Epoch 144/600\n",
      "0s - loss: 0.0624 - acc: 0.8536 - val_loss: 0.0644 - val_acc: 0.8174\n",
      "Epoch 145/600\n",
      "0s - loss: 0.0619 - acc: 0.8536 - val_loss: 0.0641 - val_acc: 0.8174\n",
      "Epoch 146/600\n",
      "0s - loss: 0.0615 - acc: 0.8565 - val_loss: 0.0638 - val_acc: 0.8217\n",
      "Epoch 147/600\n",
      "0s - loss: 0.0611 - acc: 0.8580 - val_loss: 0.0633 - val_acc: 0.8174\n",
      "Epoch 148/600\n",
      "0s - loss: 0.0607 - acc: 0.8551 - val_loss: 0.0631 - val_acc: 0.8174\n",
      "Epoch 149/600\n",
      "0s - loss: 0.0603 - acc: 0.8551 - val_loss: 0.0626 - val_acc: 0.8174\n",
      "Epoch 150/600\n",
      "0s - loss: 0.0599 - acc: 0.8551 - val_loss: 0.0624 - val_acc: 0.8174\n",
      "Epoch 151/600\n",
      "0s - loss: 0.0595 - acc: 0.8551 - val_loss: 0.0619 - val_acc: 0.8261\n",
      "Epoch 152/600\n",
      "0s - loss: 0.0591 - acc: 0.8536 - val_loss: 0.0616 - val_acc: 0.8304\n",
      "Epoch 153/600\n",
      "0s - loss: 0.0587 - acc: 0.8565 - val_loss: 0.0613 - val_acc: 0.8304\n",
      "Epoch 154/600\n",
      "0s - loss: 0.0584 - acc: 0.8565 - val_loss: 0.0610 - val_acc: 0.8304\n",
      "Epoch 155/600\n",
      "0s - loss: 0.0580 - acc: 0.8565 - val_loss: 0.0606 - val_acc: 0.8261\n",
      "Epoch 156/600\n",
      "0s - loss: 0.0576 - acc: 0.8551 - val_loss: 0.0602 - val_acc: 0.8261\n",
      "Epoch 157/600\n",
      "0s - loss: 0.0572 - acc: 0.8565 - val_loss: 0.0600 - val_acc: 0.8304\n",
      "Epoch 158/600\n",
      "0s - loss: 0.0569 - acc: 0.8565 - val_loss: 0.0595 - val_acc: 0.8261\n",
      "Epoch 159/600\n",
      "0s - loss: 0.0565 - acc: 0.8565 - val_loss: 0.0593 - val_acc: 0.8261\n",
      "Epoch 160/600\n",
      "0s - loss: 0.0562 - acc: 0.8565 - val_loss: 0.0590 - val_acc: 0.8261\n",
      "Epoch 161/600\n",
      "0s - loss: 0.0558 - acc: 0.8580 - val_loss: 0.0587 - val_acc: 0.8261\n",
      "Epoch 162/600\n",
      "0s - loss: 0.0555 - acc: 0.8580 - val_loss: 0.0584 - val_acc: 0.8261\n",
      "Epoch 163/600\n",
      "0s - loss: 0.0551 - acc: 0.8623 - val_loss: 0.0582 - val_acc: 0.8261\n",
      "Epoch 164/600\n",
      "0s - loss: 0.0548 - acc: 0.8623 - val_loss: 0.0578 - val_acc: 0.8348\n",
      "Epoch 165/600\n",
      "0s - loss: 0.0545 - acc: 0.8623 - val_loss: 0.0575 - val_acc: 0.8304\n",
      "Epoch 166/600\n",
      "0s - loss: 0.0541 - acc: 0.8638 - val_loss: 0.0572 - val_acc: 0.8304\n",
      "Epoch 167/600\n",
      "0s - loss: 0.0538 - acc: 0.8638 - val_loss: 0.0570 - val_acc: 0.8304\n",
      "Epoch 168/600\n",
      "0s - loss: 0.0534 - acc: 0.8623 - val_loss: 0.0567 - val_acc: 0.8217\n",
      "Epoch 169/600\n",
      "0s - loss: 0.0531 - acc: 0.8652 - val_loss: 0.0564 - val_acc: 0.8261\n",
      "Epoch 170/600\n",
      "0s - loss: 0.0528 - acc: 0.8638 - val_loss: 0.0562 - val_acc: 0.8261\n",
      "Epoch 171/600\n",
      "0s - loss: 0.0525 - acc: 0.8652 - val_loss: 0.0559 - val_acc: 0.8261\n",
      "Epoch 172/600\n",
      "0s - loss: 0.0522 - acc: 0.8667 - val_loss: 0.0557 - val_acc: 0.8261\n",
      "Epoch 173/600\n",
      "0s - loss: 0.0519 - acc: 0.8710 - val_loss: 0.0555 - val_acc: 0.8261\n",
      "Epoch 174/600\n",
      "0s - loss: 0.0515 - acc: 0.8710 - val_loss: 0.0553 - val_acc: 0.8261\n",
      "Epoch 175/600\n",
      "0s - loss: 0.0512 - acc: 0.8710 - val_loss: 0.0551 - val_acc: 0.8261\n",
      "Epoch 176/600\n",
      "0s - loss: 0.0509 - acc: 0.8710 - val_loss: 0.0548 - val_acc: 0.8261\n",
      "Epoch 177/600\n",
      "0s - loss: 0.0507 - acc: 0.8710 - val_loss: 0.0544 - val_acc: 0.8304\n",
      "Epoch 178/600\n",
      "0s - loss: 0.0504 - acc: 0.8725 - val_loss: 0.0542 - val_acc: 0.8304\n",
      "Epoch 179/600\n",
      "0s - loss: 0.0501 - acc: 0.8725 - val_loss: 0.0539 - val_acc: 0.8304\n",
      "Epoch 180/600\n",
      "0s - loss: 0.0499 - acc: 0.8725 - val_loss: 0.0537 - val_acc: 0.8348\n",
      "Epoch 181/600\n",
      "0s - loss: 0.0496 - acc: 0.8710 - val_loss: 0.0536 - val_acc: 0.8348\n",
      "Epoch 182/600\n",
      "0s - loss: 0.0493 - acc: 0.8725 - val_loss: 0.0535 - val_acc: 0.8348\n",
      "Epoch 183/600\n",
      "0s - loss: 0.0491 - acc: 0.8710 - val_loss: 0.0532 - val_acc: 0.8304\n",
      "Epoch 184/600\n",
      "0s - loss: 0.0488 - acc: 0.8710 - val_loss: 0.0531 - val_acc: 0.8304\n",
      "Epoch 185/600\n",
      "0s - loss: 0.0485 - acc: 0.8725 - val_loss: 0.0529 - val_acc: 0.8304\n",
      "Epoch 186/600\n",
      "0s - loss: 0.0483 - acc: 0.8725 - val_loss: 0.0526 - val_acc: 0.8348\n",
      "Epoch 187/600\n",
      "0s - loss: 0.0481 - acc: 0.8739 - val_loss: 0.0526 - val_acc: 0.8304\n",
      "Epoch 188/600\n",
      "0s - loss: 0.0478 - acc: 0.8710 - val_loss: 0.0523 - val_acc: 0.8348\n",
      "Epoch 189/600\n",
      "0s - loss: 0.0476 - acc: 0.8739 - val_loss: 0.0521 - val_acc: 0.8348\n",
      "Epoch 190/600\n",
      "0s - loss: 0.0474 - acc: 0.8754 - val_loss: 0.0521 - val_acc: 0.8304\n",
      "Epoch 191/600\n",
      "0s - loss: 0.0472 - acc: 0.8725 - val_loss: 0.0519 - val_acc: 0.8304\n",
      "Epoch 192/600\n",
      "0s - loss: 0.0469 - acc: 0.8754 - val_loss: 0.0517 - val_acc: 0.8391\n",
      "Epoch 193/600\n",
      "0s - loss: 0.0467 - acc: 0.8754 - val_loss: 0.0513 - val_acc: 0.8391\n",
      "Epoch 194/600\n",
      "0s - loss: 0.0465 - acc: 0.8768 - val_loss: 0.0511 - val_acc: 0.8435\n",
      "Epoch 195/600\n",
      "0s - loss: 0.0463 - acc: 0.8768 - val_loss: 0.0510 - val_acc: 0.8391\n",
      "Epoch 196/600\n",
      "0s - loss: 0.0461 - acc: 0.8797 - val_loss: 0.0509 - val_acc: 0.8391\n",
      "Epoch 197/600\n",
      "0s - loss: 0.0459 - acc: 0.8797 - val_loss: 0.0507 - val_acc: 0.8391\n",
      "Epoch 198/600\n",
      "0s - loss: 0.0457 - acc: 0.8841 - val_loss: 0.0508 - val_acc: 0.8391\n",
      "Epoch 199/600\n",
      "0s - loss: 0.0455 - acc: 0.8797 - val_loss: 0.0506 - val_acc: 0.8391\n",
      "Epoch 200/600\n",
      "0s - loss: 0.0453 - acc: 0.8826 - val_loss: 0.0505 - val_acc: 0.8391\n",
      "Epoch 201/600\n",
      "0s - loss: 0.0452 - acc: 0.8812 - val_loss: 0.0502 - val_acc: 0.8435\n",
      "Epoch 202/600\n",
      "0s - loss: 0.0450 - acc: 0.8884 - val_loss: 0.0503 - val_acc: 0.8435\n",
      "Epoch 203/600\n",
      "0s - loss: 0.0448 - acc: 0.8870 - val_loss: 0.0501 - val_acc: 0.8435\n",
      "Epoch 204/600\n",
      "0s - loss: 0.0446 - acc: 0.8870 - val_loss: 0.0499 - val_acc: 0.8435\n",
      "Epoch 205/600\n",
      "0s - loss: 0.0445 - acc: 0.8884 - val_loss: 0.0499 - val_acc: 0.8435\n",
      "Epoch 206/600\n",
      "0s - loss: 0.0443 - acc: 0.8884 - val_loss: 0.0497 - val_acc: 0.8435\n",
      "Epoch 207/600\n",
      "0s - loss: 0.0441 - acc: 0.8899 - val_loss: 0.0496 - val_acc: 0.8478\n",
      "Epoch 208/600\n",
      "0s - loss: 0.0440 - acc: 0.8942 - val_loss: 0.0495 - val_acc: 0.8478\n",
      "Epoch 209/600\n",
      "0s - loss: 0.0438 - acc: 0.8942 - val_loss: 0.0492 - val_acc: 0.8522\n",
      "Epoch 210/600\n",
      "0s - loss: 0.0436 - acc: 0.8942 - val_loss: 0.0492 - val_acc: 0.8478\n",
      "Epoch 211/600\n",
      "0s - loss: 0.0435 - acc: 0.8957 - val_loss: 0.0491 - val_acc: 0.8522\n",
      "Epoch 212/600\n",
      "0s - loss: 0.0433 - acc: 0.8928 - val_loss: 0.0489 - val_acc: 0.8522\n",
      "Epoch 213/600\n",
      "0s - loss: 0.0432 - acc: 0.8913 - val_loss: 0.0489 - val_acc: 0.8522\n",
      "Epoch 214/600\n",
      "0s - loss: 0.0430 - acc: 0.8928 - val_loss: 0.0489 - val_acc: 0.8522\n",
      "Epoch 215/600\n",
      "0s - loss: 0.0429 - acc: 0.8928 - val_loss: 0.0489 - val_acc: 0.8565\n",
      "Epoch 216/600\n",
      "0s - loss: 0.0427 - acc: 0.8928 - val_loss: 0.0488 - val_acc: 0.8565\n",
      "Epoch 217/600\n",
      "0s - loss: 0.0426 - acc: 0.8913 - val_loss: 0.0485 - val_acc: 0.8565\n",
      "Epoch 218/600\n",
      "0s - loss: 0.0424 - acc: 0.8928 - val_loss: 0.0483 - val_acc: 0.8565\n",
      "Epoch 219/600\n",
      "0s - loss: 0.0424 - acc: 0.8942 - val_loss: 0.0485 - val_acc: 0.8565\n",
      "Epoch 220/600\n",
      "0s - loss: 0.0422 - acc: 0.8928 - val_loss: 0.0484 - val_acc: 0.8565\n",
      "Epoch 221/600\n",
      "0s - loss: 0.0421 - acc: 0.8913 - val_loss: 0.0482 - val_acc: 0.8565\n",
      "Epoch 222/600\n",
      "0s - loss: 0.0419 - acc: 0.8913 - val_loss: 0.0481 - val_acc: 0.8565\n",
      "Epoch 223/600\n",
      "0s - loss: 0.0418 - acc: 0.8928 - val_loss: 0.0478 - val_acc: 0.8609\n",
      "Epoch 224/600\n",
      "0s - loss: 0.0417 - acc: 0.8942 - val_loss: 0.0480 - val_acc: 0.8565\n",
      "Epoch 225/600\n",
      "0s - loss: 0.0416 - acc: 0.8928 - val_loss: 0.0479 - val_acc: 0.8609\n",
      "Epoch 226/600\n",
      "0s - loss: 0.0415 - acc: 0.8928 - val_loss: 0.0478 - val_acc: 0.8609\n",
      "Epoch 227/600\n",
      "0s - loss: 0.0414 - acc: 0.8942 - val_loss: 0.0477 - val_acc: 0.8609\n",
      "Epoch 228/600\n",
      "0s - loss: 0.0412 - acc: 0.8942 - val_loss: 0.0475 - val_acc: 0.8609\n",
      "Epoch 229/600\n",
      "0s - loss: 0.0411 - acc: 0.8957 - val_loss: 0.0473 - val_acc: 0.8609\n",
      "Epoch 230/600\n",
      "0s - loss: 0.0410 - acc: 0.8957 - val_loss: 0.0473 - val_acc: 0.8609\n",
      "Epoch 231/600\n",
      "0s - loss: 0.0409 - acc: 0.8957 - val_loss: 0.0473 - val_acc: 0.8609\n",
      "Epoch 232/600\n",
      "0s - loss: 0.0408 - acc: 0.8957 - val_loss: 0.0474 - val_acc: 0.8565\n",
      "Epoch 233/600\n",
      "0s - loss: 0.0407 - acc: 0.8957 - val_loss: 0.0473 - val_acc: 0.8565\n",
      "Epoch 234/600\n",
      "0s - loss: 0.0405 - acc: 0.8957 - val_loss: 0.0472 - val_acc: 0.8565\n",
      "Epoch 235/600\n",
      "0s - loss: 0.0404 - acc: 0.8942 - val_loss: 0.0472 - val_acc: 0.8565\n",
      "Epoch 236/600\n",
      "0s - loss: 0.0403 - acc: 0.8957 - val_loss: 0.0473 - val_acc: 0.8565\n",
      "Epoch 237/600\n",
      "0s - loss: 0.0402 - acc: 0.8928 - val_loss: 0.0471 - val_acc: 0.8565\n",
      "Epoch 238/600\n",
      "0s - loss: 0.0401 - acc: 0.8928 - val_loss: 0.0469 - val_acc: 0.8609\n",
      "Epoch 239/600\n",
      "0s - loss: 0.0400 - acc: 0.8928 - val_loss: 0.0467 - val_acc: 0.8565\n",
      "Epoch 240/600\n",
      "0s - loss: 0.0399 - acc: 0.8957 - val_loss: 0.0466 - val_acc: 0.8565\n",
      "Epoch 241/600\n",
      "0s - loss: 0.0398 - acc: 0.8942 - val_loss: 0.0465 - val_acc: 0.8565\n",
      "Epoch 242/600\n",
      "0s - loss: 0.0397 - acc: 0.8942 - val_loss: 0.0465 - val_acc: 0.8565\n",
      "Epoch 243/600\n",
      "0s - loss: 0.0396 - acc: 0.8942 - val_loss: 0.0463 - val_acc: 0.8565\n",
      "Epoch 244/600\n",
      "0s - loss: 0.0395 - acc: 0.8957 - val_loss: 0.0461 - val_acc: 0.8565\n",
      "Epoch 245/600\n",
      "0s - loss: 0.0394 - acc: 0.8957 - val_loss: 0.0459 - val_acc: 0.8609\n",
      "Epoch 246/600\n",
      "0s - loss: 0.0393 - acc: 0.8942 - val_loss: 0.0460 - val_acc: 0.8565\n",
      "Epoch 247/600\n",
      "0s - loss: 0.0392 - acc: 0.8942 - val_loss: 0.0460 - val_acc: 0.8565\n",
      "Epoch 248/600\n",
      "0s - loss: 0.0392 - acc: 0.8942 - val_loss: 0.0457 - val_acc: 0.8652\n",
      "Epoch 249/600\n",
      "0s - loss: 0.0390 - acc: 0.8928 - val_loss: 0.0459 - val_acc: 0.8609\n",
      "Epoch 250/600\n",
      "0s - loss: 0.0390 - acc: 0.8942 - val_loss: 0.0459 - val_acc: 0.8565\n",
      "Epoch 251/600\n",
      "0s - loss: 0.0389 - acc: 0.8957 - val_loss: 0.0459 - val_acc: 0.8565\n",
      "Epoch 252/600\n",
      "0s - loss: 0.0388 - acc: 0.8971 - val_loss: 0.0456 - val_acc: 0.8652\n",
      "Epoch 253/600\n",
      "0s - loss: 0.0387 - acc: 0.8928 - val_loss: 0.0457 - val_acc: 0.8609\n",
      "Epoch 254/600\n",
      "0s - loss: 0.0387 - acc: 0.8971 - val_loss: 0.0456 - val_acc: 0.8652\n",
      "Epoch 255/600\n",
      "0s - loss: 0.0386 - acc: 0.8971 - val_loss: 0.0456 - val_acc: 0.8609\n",
      "Epoch 256/600\n",
      "0s - loss: 0.0386 - acc: 0.8957 - val_loss: 0.0453 - val_acc: 0.8696\n",
      "Epoch 257/600\n",
      "0s - loss: 0.0385 - acc: 0.8928 - val_loss: 0.0453 - val_acc: 0.8696\n",
      "Epoch 258/600\n",
      "0s - loss: 0.0384 - acc: 0.8942 - val_loss: 0.0453 - val_acc: 0.8696\n",
      "Epoch 259/600\n",
      "0s - loss: 0.0383 - acc: 0.8942 - val_loss: 0.0452 - val_acc: 0.8696\n",
      "Epoch 260/600\n",
      "0s - loss: 0.0383 - acc: 0.8971 - val_loss: 0.0452 - val_acc: 0.8696\n",
      "Epoch 261/600\n",
      "0s - loss: 0.0382 - acc: 0.8986 - val_loss: 0.0451 - val_acc: 0.8696\n",
      "Epoch 262/600\n",
      "0s - loss: 0.0382 - acc: 0.8986 - val_loss: 0.0450 - val_acc: 0.8696\n",
      "Epoch 263/600\n",
      "0s - loss: 0.0381 - acc: 0.8986 - val_loss: 0.0450 - val_acc: 0.8696\n",
      "Epoch 264/600\n",
      "0s - loss: 0.0380 - acc: 0.8986 - val_loss: 0.0451 - val_acc: 0.8696\n",
      "Epoch 265/600\n",
      "0s - loss: 0.0380 - acc: 0.8986 - val_loss: 0.0451 - val_acc: 0.8652\n",
      "Epoch 266/600\n",
      "0s - loss: 0.0379 - acc: 0.8942 - val_loss: 0.0451 - val_acc: 0.8609\n",
      "Epoch 267/600\n",
      "0s - loss: 0.0379 - acc: 0.8942 - val_loss: 0.0451 - val_acc: 0.8609\n",
      "Epoch 268/600\n",
      "0s - loss: 0.0378 - acc: 0.8942 - val_loss: 0.0447 - val_acc: 0.8696\n",
      "Epoch 269/600\n",
      "0s - loss: 0.0377 - acc: 0.8986 - val_loss: 0.0448 - val_acc: 0.8652\n",
      "Epoch 270/600\n",
      "0s - loss: 0.0377 - acc: 0.8957 - val_loss: 0.0446 - val_acc: 0.8696\n",
      "Epoch 271/600\n",
      "0s - loss: 0.0376 - acc: 0.8986 - val_loss: 0.0447 - val_acc: 0.8609\n",
      "Epoch 272/600\n",
      "0s - loss: 0.0376 - acc: 0.8971 - val_loss: 0.0447 - val_acc: 0.8609\n",
      "Epoch 273/600\n",
      "0s - loss: 0.0376 - acc: 0.8957 - val_loss: 0.0446 - val_acc: 0.8609\n",
      "Epoch 274/600\n",
      "0s - loss: 0.0376 - acc: 0.8971 - val_loss: 0.0448 - val_acc: 0.8609\n",
      "Epoch 275/600\n",
      "0s - loss: 0.0375 - acc: 0.8957 - val_loss: 0.0446 - val_acc: 0.8609\n",
      "Epoch 276/600\n",
      "0s - loss: 0.0374 - acc: 0.8971 - val_loss: 0.0445 - val_acc: 0.8609\n",
      "Epoch 277/600\n",
      "0s - loss: 0.0374 - acc: 0.8971 - val_loss: 0.0443 - val_acc: 0.8609\n",
      "Epoch 278/600\n",
      "0s - loss: 0.0374 - acc: 0.8986 - val_loss: 0.0445 - val_acc: 0.8609\n",
      "Epoch 279/600\n",
      "0s - loss: 0.0374 - acc: 0.8986 - val_loss: 0.0446 - val_acc: 0.8609\n",
      "Epoch 280/600\n",
      "0s - loss: 0.0373 - acc: 0.8957 - val_loss: 0.0445 - val_acc: 0.8609\n",
      "Epoch 281/600\n",
      "0s - loss: 0.0373 - acc: 0.8986 - val_loss: 0.0445 - val_acc: 0.8609\n",
      "Epoch 282/600\n",
      "0s - loss: 0.0373 - acc: 0.8957 - val_loss: 0.0445 - val_acc: 0.8609\n",
      "Epoch 283/600\n",
      "0s - loss: 0.0372 - acc: 0.8957 - val_loss: 0.0444 - val_acc: 0.8609\n",
      "Epoch 284/600\n",
      "0s - loss: 0.0372 - acc: 0.8971 - val_loss: 0.0442 - val_acc: 0.8609\n",
      "Epoch 285/600\n",
      "0s - loss: 0.0371 - acc: 0.9000 - val_loss: 0.0440 - val_acc: 0.8609\n",
      "Epoch 286/600\n",
      "0s - loss: 0.0371 - acc: 0.9014 - val_loss: 0.0440 - val_acc: 0.8609\n",
      "Epoch 287/600\n",
      "0s - loss: 0.0370 - acc: 0.9014 - val_loss: 0.0439 - val_acc: 0.8652\n",
      "Epoch 288/600\n",
      "0s - loss: 0.0370 - acc: 0.9029 - val_loss: 0.0438 - val_acc: 0.8652\n",
      "Epoch 289/600\n",
      "0s - loss: 0.0370 - acc: 0.9000 - val_loss: 0.0436 - val_acc: 0.8652\n",
      "Epoch 290/600\n",
      "0s - loss: 0.0370 - acc: 0.9000 - val_loss: 0.0436 - val_acc: 0.8652\n",
      "Epoch 291/600\n",
      "0s - loss: 0.0369 - acc: 0.9029 - val_loss: 0.0437 - val_acc: 0.8652\n",
      "Epoch 292/600\n",
      "0s - loss: 0.0369 - acc: 0.9000 - val_loss: 0.0438 - val_acc: 0.8652\n",
      "Epoch 293/600\n",
      "0s - loss: 0.0368 - acc: 0.8986 - val_loss: 0.0438 - val_acc: 0.8652\n",
      "Epoch 294/600\n",
      "0s - loss: 0.0368 - acc: 0.9000 - val_loss: 0.0436 - val_acc: 0.8652\n",
      "Epoch 295/600\n",
      "0s - loss: 0.0368 - acc: 0.9000 - val_loss: 0.0436 - val_acc: 0.8652\n",
      "Epoch 296/600\n",
      "0s - loss: 0.0368 - acc: 0.8986 - val_loss: 0.0438 - val_acc: 0.8652\n",
      "Epoch 297/600\n",
      "0s - loss: 0.0367 - acc: 0.9000 - val_loss: 0.0437 - val_acc: 0.8652\n",
      "Epoch 298/600\n",
      "0s - loss: 0.0367 - acc: 0.8971 - val_loss: 0.0436 - val_acc: 0.8652\n",
      "Epoch 299/600\n",
      "0s - loss: 0.0367 - acc: 0.8971 - val_loss: 0.0434 - val_acc: 0.8652\n",
      "Epoch 300/600\n",
      "0s - loss: 0.0367 - acc: 0.8986 - val_loss: 0.0433 - val_acc: 0.8652\n",
      "Epoch 301/600\n",
      "0s - loss: 0.0366 - acc: 0.9000 - val_loss: 0.0432 - val_acc: 0.8652\n",
      "Epoch 302/600\n",
      "0s - loss: 0.0366 - acc: 0.9043 - val_loss: 0.0433 - val_acc: 0.8652\n",
      "Epoch 303/600\n",
      "0s - loss: 0.0366 - acc: 0.9014 - val_loss: 0.0434 - val_acc: 0.8652\n",
      "Epoch 304/600\n",
      "0s - loss: 0.0366 - acc: 0.8971 - val_loss: 0.0433 - val_acc: 0.8652\n",
      "Epoch 305/600\n",
      "0s - loss: 0.0365 - acc: 0.9014 - val_loss: 0.0432 - val_acc: 0.8652\n",
      "Epoch 306/600\n",
      "0s - loss: 0.0365 - acc: 0.9043 - val_loss: 0.0433 - val_acc: 0.8652\n",
      "Epoch 307/600\n",
      "0s - loss: 0.0365 - acc: 0.8986 - val_loss: 0.0433 - val_acc: 0.8652\n",
      "Epoch 308/600\n",
      "0s - loss: 0.0364 - acc: 0.8986 - val_loss: 0.0434 - val_acc: 0.8652\n",
      "Epoch 309/600\n",
      "0s - loss: 0.0364 - acc: 0.8971 - val_loss: 0.0433 - val_acc: 0.8696\n",
      "Epoch 310/600\n",
      "0s - loss: 0.0364 - acc: 0.8971 - val_loss: 0.0431 - val_acc: 0.8652\n",
      "Epoch 311/600\n",
      "0s - loss: 0.0364 - acc: 0.9014 - val_loss: 0.0432 - val_acc: 0.8696\n",
      "Epoch 312/600\n",
      "0s - loss: 0.0364 - acc: 0.8971 - val_loss: 0.0434 - val_acc: 0.8696\n",
      "Epoch 313/600\n",
      "0s - loss: 0.0364 - acc: 0.8957 - val_loss: 0.0431 - val_acc: 0.8696\n",
      "Epoch 314/600\n",
      "0s - loss: 0.0363 - acc: 0.9000 - val_loss: 0.0432 - val_acc: 0.8696\n",
      "Epoch 315/600\n",
      "0s - loss: 0.0363 - acc: 0.8971 - val_loss: 0.0433 - val_acc: 0.8696\n",
      "Epoch 316/600\n",
      "0s - loss: 0.0363 - acc: 0.8957 - val_loss: 0.0431 - val_acc: 0.8696\n",
      "Epoch 317/600\n",
      "0s - loss: 0.0363 - acc: 0.9000 - val_loss: 0.0431 - val_acc: 0.8696\n",
      "Epoch 318/600\n",
      "0s - loss: 0.0363 - acc: 0.8986 - val_loss: 0.0431 - val_acc: 0.8696\n",
      "Epoch 319/600\n",
      "0s - loss: 0.0362 - acc: 0.9000 - val_loss: 0.0432 - val_acc: 0.8696\n",
      "Epoch 320/600\n",
      "0s - loss: 0.0362 - acc: 0.8971 - val_loss: 0.0431 - val_acc: 0.8696\n",
      "Epoch 321/600\n",
      "0s - loss: 0.0362 - acc: 0.8971 - val_loss: 0.0428 - val_acc: 0.8696\n",
      "Epoch 322/600\n",
      "0s - loss: 0.0362 - acc: 0.9029 - val_loss: 0.0428 - val_acc: 0.8696\n",
      "Epoch 323/600\n",
      "0s - loss: 0.0362 - acc: 0.9043 - val_loss: 0.0428 - val_acc: 0.8696\n",
      "Epoch 324/600\n",
      "0s - loss: 0.0362 - acc: 0.9014 - val_loss: 0.0430 - val_acc: 0.8696\n",
      "Epoch 325/600\n",
      "0s - loss: 0.0362 - acc: 0.8986 - val_loss: 0.0428 - val_acc: 0.8696\n",
      "Epoch 326/600\n",
      "0s - loss: 0.0361 - acc: 0.9029 - val_loss: 0.0427 - val_acc: 0.8696\n",
      "Epoch 327/600\n",
      "0s - loss: 0.0361 - acc: 0.9029 - val_loss: 0.0428 - val_acc: 0.8696\n",
      "Epoch 328/600\n",
      "0s - loss: 0.0361 - acc: 0.8986 - val_loss: 0.0428 - val_acc: 0.8696\n",
      "Epoch 329/600\n",
      "0s - loss: 0.0361 - acc: 0.9000 - val_loss: 0.0428 - val_acc: 0.8696\n",
      "Epoch 330/600\n",
      "0s - loss: 0.0361 - acc: 0.8971 - val_loss: 0.0427 - val_acc: 0.8696\n",
      "Epoch 331/600\n",
      "0s - loss: 0.0361 - acc: 0.9014 - val_loss: 0.0427 - val_acc: 0.8696\n",
      "Epoch 332/600\n",
      "0s - loss: 0.0361 - acc: 0.9029 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 333/600\n",
      "0s - loss: 0.0361 - acc: 0.9043 - val_loss: 0.0427 - val_acc: 0.8696\n",
      "Epoch 334/600\n",
      "0s - loss: 0.0360 - acc: 0.9029 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 335/600\n",
      "0s - loss: 0.0360 - acc: 0.9043 - val_loss: 0.0427 - val_acc: 0.8696\n",
      "Epoch 336/600\n",
      "0s - loss: 0.0360 - acc: 0.8986 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 337/600\n",
      "0s - loss: 0.0360 - acc: 0.9000 - val_loss: 0.0425 - val_acc: 0.8696\n",
      "Epoch 338/600\n",
      "0s - loss: 0.0360 - acc: 0.9043 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 339/600\n",
      "0s - loss: 0.0360 - acc: 0.9000 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 340/600\n",
      "0s - loss: 0.0360 - acc: 0.9029 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 341/600\n",
      "0s - loss: 0.0360 - acc: 0.9029 - val_loss: 0.0425 - val_acc: 0.8696\n",
      "Epoch 342/600\n",
      "0s - loss: 0.0360 - acc: 0.9043 - val_loss: 0.0424 - val_acc: 0.8696\n",
      "Epoch 343/600\n",
      "0s - loss: 0.0360 - acc: 0.9058 - val_loss: 0.0424 - val_acc: 0.8696\n",
      "Epoch 344/600\n",
      "0s - loss: 0.0360 - acc: 0.9043 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 345/600\n",
      "0s - loss: 0.0359 - acc: 0.9000 - val_loss: 0.0425 - val_acc: 0.8696\n",
      "Epoch 346/600\n",
      "0s - loss: 0.0359 - acc: 0.9043 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 347/600\n",
      "0s - loss: 0.0359 - acc: 0.9000 - val_loss: 0.0427 - val_acc: 0.8696\n",
      "Epoch 348/600\n",
      "0s - loss: 0.0359 - acc: 0.8971 - val_loss: 0.0427 - val_acc: 0.8696\n",
      "Epoch 349/600\n",
      "0s - loss: 0.0359 - acc: 0.8986 - val_loss: 0.0425 - val_acc: 0.8696\n",
      "Epoch 350/600\n",
      "0s - loss: 0.0359 - acc: 0.9000 - val_loss: 0.0424 - val_acc: 0.8696\n",
      "Epoch 351/600\n",
      "0s - loss: 0.0359 - acc: 0.9043 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 352/600\n",
      "0s - loss: 0.0359 - acc: 0.8986 - val_loss: 0.0426 - val_acc: 0.8652\n",
      "Epoch 353/600\n",
      "0s - loss: 0.0359 - acc: 0.8986 - val_loss: 0.0424 - val_acc: 0.8696\n",
      "Epoch 354/600\n",
      "0s - loss: 0.0358 - acc: 0.9000 - val_loss: 0.0425 - val_acc: 0.8652\n",
      "Epoch 355/600\n",
      "0s - loss: 0.0358 - acc: 0.8986 - val_loss: 0.0426 - val_acc: 0.8652\n",
      "Epoch 356/600\n",
      "0s - loss: 0.0358 - acc: 0.8971 - val_loss: 0.0425 - val_acc: 0.8652\n",
      "Epoch 357/600\n",
      "0s - loss: 0.0358 - acc: 0.8971 - val_loss: 0.0425 - val_acc: 0.8652\n",
      "Epoch 358/600\n",
      "0s - loss: 0.0359 - acc: 0.9000 - val_loss: 0.0426 - val_acc: 0.8696\n",
      "Epoch 359/600\n",
      "0s - loss: 0.0358 - acc: 0.8971 - val_loss: 0.0424 - val_acc: 0.8652\n",
      "Epoch 360/600\n",
      "0s - loss: 0.0358 - acc: 0.9014 - val_loss: 0.0422 - val_acc: 0.8652\n",
      "Epoch 361/600\n",
      "0s - loss: 0.0358 - acc: 0.9043 - val_loss: 0.0423 - val_acc: 0.8652\n",
      "Epoch 362/600\n",
      "0s - loss: 0.0358 - acc: 0.9029 - val_loss: 0.0423 - val_acc: 0.8652\n",
      "Epoch 363/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0423 - val_acc: 0.8652\n",
      "Epoch 364/600\n",
      "0s - loss: 0.0358 - acc: 0.9043 - val_loss: 0.0422 - val_acc: 0.8652\n",
      "Epoch 365/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0422 - val_acc: 0.8652\n",
      "Epoch 366/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0422 - val_acc: 0.8652\n",
      "Epoch 367/600\n",
      "0s - loss: 0.0357 - acc: 0.9029 - val_loss: 0.0421 - val_acc: 0.8652\n",
      "Epoch 368/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0420 - val_acc: 0.8696\n",
      "Epoch 369/600\n",
      "0s - loss: 0.0357 - acc: 0.9072 - val_loss: 0.0422 - val_acc: 0.8652\n",
      "Epoch 370/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0421 - val_acc: 0.8652\n",
      "Epoch 371/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0422 - val_acc: 0.8652\n",
      "Epoch 372/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0422 - val_acc: 0.8696\n",
      "Epoch 373/600\n",
      "0s - loss: 0.0357 - acc: 0.9029 - val_loss: 0.0421 - val_acc: 0.8696\n",
      "Epoch 374/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0421 - val_acc: 0.8739\n",
      "Epoch 375/600\n",
      "0s - loss: 0.0357 - acc: 0.9043 - val_loss: 0.0420 - val_acc: 0.8696\n",
      "Epoch 376/600\n",
      "0s - loss: 0.0356 - acc: 0.9043 - val_loss: 0.0420 - val_acc: 0.8739\n",
      "Epoch 377/600\n",
      "0s - loss: 0.0356 - acc: 0.9043 - val_loss: 0.0421 - val_acc: 0.8739\n",
      "Epoch 378/600\n",
      "0s - loss: 0.0356 - acc: 0.9029 - val_loss: 0.0421 - val_acc: 0.8739\n",
      "Epoch 379/600\n",
      "0s - loss: 0.0356 - acc: 0.9043 - val_loss: 0.0420 - val_acc: 0.8739\n",
      "Epoch 380/600\n",
      "0s - loss: 0.0356 - acc: 0.9029 - val_loss: 0.0420 - val_acc: 0.8739\n",
      "Epoch 381/600\n",
      "0s - loss: 0.0356 - acc: 0.9029 - val_loss: 0.0421 - val_acc: 0.8739\n",
      "Epoch 382/600\n",
      "0s - loss: 0.0356 - acc: 0.9029 - val_loss: 0.0420 - val_acc: 0.8739\n",
      "Epoch 383/600\n",
      "0s - loss: 0.0356 - acc: 0.9043 - val_loss: 0.0421 - val_acc: 0.8739\n",
      "Epoch 384/600\n",
      "0s - loss: 0.0356 - acc: 0.9014 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 385/600\n",
      "0s - loss: 0.0355 - acc: 0.9072 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 386/600\n",
      "0s - loss: 0.0357 - acc: 0.9014 - val_loss: 0.0418 - val_acc: 0.8783\n",
      "Epoch 387/600\n",
      "0s - loss: 0.0355 - acc: 0.9072 - val_loss: 0.0418 - val_acc: 0.8783\n",
      "Epoch 388/600\n",
      "0s - loss: 0.0355 - acc: 0.9072 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 389/600\n",
      "0s - loss: 0.0355 - acc: 0.9029 - val_loss: 0.0420 - val_acc: 0.8739\n",
      "Epoch 390/600\n",
      "0s - loss: 0.0355 - acc: 0.9000 - val_loss: 0.0420 - val_acc: 0.8739\n",
      "Epoch 391/600\n",
      "0s - loss: 0.0355 - acc: 0.8986 - val_loss: 0.0418 - val_acc: 0.8739\n",
      "Epoch 392/600\n",
      "0s - loss: 0.0355 - acc: 0.9029 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 393/600\n",
      "0s - loss: 0.0355 - acc: 0.9058 - val_loss: 0.0418 - val_acc: 0.8783\n",
      "Epoch 394/600\n",
      "0s - loss: 0.0355 - acc: 0.9058 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 395/600\n",
      "0s - loss: 0.0355 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8783\n",
      "Epoch 396/600\n",
      "0s - loss: 0.0355 - acc: 0.9058 - val_loss: 0.0417 - val_acc: 0.8783\n",
      "Epoch 397/600\n",
      "0s - loss: 0.0355 - acc: 0.9043 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 398/600\n",
      "0s - loss: 0.0355 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 399/600\n",
      "0s - loss: 0.0355 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 400/600\n",
      "0s - loss: 0.0355 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 401/600\n",
      "0s - loss: 0.0355 - acc: 0.9029 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 402/600\n",
      "0s - loss: 0.0355 - acc: 0.9043 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 403/600\n",
      "0s - loss: 0.0355 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 404/600\n",
      "0s - loss: 0.0354 - acc: 0.9058 - val_loss: 0.0418 - val_acc: 0.8783\n",
      "Epoch 405/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0418 - val_acc: 0.8739\n",
      "Epoch 406/600\n",
      "0s - loss: 0.0354 - acc: 0.9029 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 407/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 408/600\n",
      "0s - loss: 0.0355 - acc: 0.9014 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 409/600\n",
      "0s - loss: 0.0354 - acc: 0.9029 - val_loss: 0.0417 - val_acc: 0.8783\n",
      "Epoch 410/600\n",
      "0s - loss: 0.0355 - acc: 0.9029 - val_loss: 0.0418 - val_acc: 0.8739\n",
      "Epoch 411/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0418 - val_acc: 0.8739\n",
      "Epoch 412/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8783\n",
      "Epoch 413/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8783\n",
      "Epoch 414/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 415/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 416/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0418 - val_acc: 0.8826\n",
      "Epoch 417/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0418 - val_acc: 0.8783\n",
      "Epoch 418/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 419/600\n",
      "0s - loss: 0.0354 - acc: 0.9014 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 420/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 421/600\n",
      "0s - loss: 0.0354 - acc: 0.9029 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 422/600\n",
      "0s - loss: 0.0354 - acc: 0.9043 - val_loss: 0.0419 - val_acc: 0.8739\n",
      "Epoch 423/600\n",
      "0s - loss: 0.0353 - acc: 0.9014 - val_loss: 0.0418 - val_acc: 0.8826\n",
      "Epoch 424/600\n",
      "0s - loss: 0.0353 - acc: 0.9029 - val_loss: 0.0418 - val_acc: 0.8739\n",
      "Epoch 425/600\n",
      "0s - loss: 0.0353 - acc: 0.9029 - val_loss: 0.0418 - val_acc: 0.8826\n",
      "Epoch 426/600\n",
      "0s - loss: 0.0353 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 427/600\n",
      "0s - loss: 0.0353 - acc: 0.9029 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 428/600\n",
      "0s - loss: 0.0353 - acc: 0.9029 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 429/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 430/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0418 - val_acc: 0.8826\n",
      "Epoch 431/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 432/600\n",
      "0s - loss: 0.0353 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 433/600\n",
      "0s - loss: 0.0353 - acc: 0.9058 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 434/600\n",
      "0s - loss: 0.0353 - acc: 0.9014 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 435/600\n",
      "0s - loss: 0.0353 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 436/600\n",
      "0s - loss: 0.0353 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 437/600\n",
      "0s - loss: 0.0353 - acc: 0.9058 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 438/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 439/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 440/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 441/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 442/600\n",
      "0s - loss: 0.0353 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 443/600\n",
      "0s - loss: 0.0352 - acc: 0.9043 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 444/600\n",
      "0s - loss: 0.0352 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 445/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 446/600\n",
      "0s - loss: 0.0353 - acc: 0.9014 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 447/600\n",
      "0s - loss: 0.0353 - acc: 0.9058 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 448/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0418 - val_acc: 0.8826\n",
      "Epoch 449/600\n",
      "0s - loss: 0.0352 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 450/600\n",
      "0s - loss: 0.0352 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 451/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0418 - val_acc: 0.8826\n",
      "Epoch 452/600\n",
      "0s - loss: 0.0352 - acc: 0.9000 - val_loss: 0.0419 - val_acc: 0.8826\n",
      "Epoch 453/600\n",
      "0s - loss: 0.0353 - acc: 0.9043 - val_loss: 0.0420 - val_acc: 0.8826\n",
      "Epoch 454/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0419 - val_acc: 0.8826\n",
      "Epoch 455/600\n",
      "0s - loss: 0.0352 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 456/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0417 - val_acc: 0.8870\n",
      "Epoch 457/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 458/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 459/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 460/600\n",
      "0s - loss: 0.0352 - acc: 0.9043 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 461/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 462/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 463/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 464/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 465/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 466/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 467/600\n",
      "0s - loss: 0.0352 - acc: 0.9043 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 468/600\n",
      "0s - loss: 0.0352 - acc: 0.9014 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 469/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 470/600\n",
      "0s - loss: 0.0351 - acc: 0.9043 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 471/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 472/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 473/600\n",
      "0s - loss: 0.0352 - acc: 0.9000 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 474/600\n",
      "0s - loss: 0.0352 - acc: 0.9029 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 475/600\n",
      "0s - loss: 0.0352 - acc: 0.9000 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 476/600\n",
      "0s - loss: 0.0352 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 477/600\n",
      "0s - loss: 0.0352 - acc: 0.9014 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 478/600\n",
      "0s - loss: 0.0351 - acc: 0.9029 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 479/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 480/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 481/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 482/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 483/600\n",
      "0s - loss: 0.0351 - acc: 0.9087 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 484/600\n",
      "0s - loss: 0.0351 - acc: 0.9087 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 485/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 486/600\n",
      "0s - loss: 0.0351 - acc: 0.9043 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 487/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 488/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 489/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 490/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 491/600\n",
      "0s - loss: 0.0351 - acc: 0.9087 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 492/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 493/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 494/600\n",
      "0s - loss: 0.0352 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 495/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 496/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 497/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0417 - val_acc: 0.8870\n",
      "Epoch 498/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0418 - val_acc: 0.8870\n",
      "Epoch 499/600\n",
      "0s - loss: 0.0351 - acc: 0.9043 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 500/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 501/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 502/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 503/600\n",
      "0s - loss: 0.0351 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 504/600\n",
      "0s - loss: 0.0351 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 505/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 506/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 507/600\n",
      "0s - loss: 0.0351 - acc: 0.9087 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 508/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 509/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 510/600\n",
      "0s - loss: 0.0351 - acc: 0.9043 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 511/600\n",
      "0s - loss: 0.0351 - acc: 0.9029 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 512/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 513/600\n",
      "0s - loss: 0.0351 - acc: 0.9101 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 514/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 515/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 516/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 517/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 518/600\n",
      "0s - loss: 0.0351 - acc: 0.9087 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 519/600\n",
      "0s - loss: 0.0351 - acc: 0.9043 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 520/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 521/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 522/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0417 - val_acc: 0.8826\n",
      "Epoch 523/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 524/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 525/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 526/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 527/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 528/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 529/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 530/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 531/600\n",
      "0s - loss: 0.0351 - acc: 0.9101 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 532/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 533/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 534/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 535/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 536/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 537/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 538/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 539/600\n",
      "0s - loss: 0.0350 - acc: 0.9101 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 540/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 541/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8870\n",
      "Epoch 542/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 543/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 544/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0415 - val_acc: 0.8870\n",
      "Epoch 545/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 546/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 547/600\n",
      "0s - loss: 0.0351 - acc: 0.9101 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 548/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 549/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 550/600\n",
      "0s - loss: 0.0350 - acc: 0.9058 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 551/600\n",
      "0s - loss: 0.0350 - acc: 0.9043 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 552/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 553/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 554/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 555/600\n",
      "0s - loss: 0.0350 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 556/600\n",
      "0s - loss: 0.0350 - acc: 0.9058 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 557/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0416 - val_acc: 0.8826\n",
      "Epoch 558/600\n",
      "0s - loss: 0.0350 - acc: 0.9043 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 559/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 560/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 561/600\n",
      "0s - loss: 0.0351 - acc: 0.9058 - val_loss: 0.0413 - val_acc: 0.8870\n",
      "Epoch 562/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 563/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 564/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 565/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 566/600\n",
      "0s - loss: 0.0351 - acc: 0.9072 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 567/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 568/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 569/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 570/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 571/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 572/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 573/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 574/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 575/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 576/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8913\n",
      "Epoch 577/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 578/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 579/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 580/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 581/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 582/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 583/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Epoch 584/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 585/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 586/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 587/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 588/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8870\n",
      "Epoch 589/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 590/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 591/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 592/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0413 - val_acc: 0.8826\n",
      "Epoch 593/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0412 - val_acc: 0.8913\n",
      "Epoch 594/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0412 - val_acc: 0.8913\n",
      "Epoch 595/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0413 - val_acc: 0.8913\n",
      "Epoch 596/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0412 - val_acc: 0.8870\n",
      "Epoch 597/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0412 - val_acc: 0.8913\n",
      "Epoch 598/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0412 - val_acc: 0.8913\n",
      "Epoch 599/600\n",
      "0s - loss: 0.0350 - acc: 0.9087 - val_loss: 0.0414 - val_acc: 0.8826\n",
      "Epoch 600/600\n",
      "0s - loss: 0.0350 - acc: 0.9072 - val_loss: 0.0415 - val_acc: 0.8826\n",
      "Train Score:  96.5032145068\n",
      "Test Score:  95.8535086655\n"
     ]
    }
   ],
   "source": [
    "#without k best features,sigmoid and rmsprop\n",
    "m1=make_model('sigmoid','rmsprop',x_train.shape[1],[x_train.shape[1],16],x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision model 1: 91\n"
     ]
    }
   ],
   "source": [
    "pr=m1.predict(x_test)\n",
    "u=0\n",
    "k=0\n",
    "tp=0\n",
    "p=0\n",
    "for u in range(len(x_test)):\n",
    "    if round(pr[u][0],1)>=0.3 and round(pr[u][0],1)<=0.8:\n",
    "        g=svmpred[u]\n",
    "    else:\n",
    "        g=round(pr[u][0],0)\n",
    "    if g!=y_test[u]:    \n",
    "        #print \"expected\",y_test[u],\"predicted:\",pr[u][0],\" \",svmpred[u]\n",
    "        k=k+1\n",
    "    if g==1:\n",
    "        p=p+1\n",
    "        if y_test[u]==1:\n",
    "            tp=tp+1\n",
    "#print \"error\",k*100/len(y_test),\"%\"\n",
    "print \"precision model 1:\",tp*100/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(690, 6)\n"
     ]
    }
   ],
   "source": [
    "print tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/miniconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, activation=\"sigmoid\", kernel_initializer=\"uniform\", input_dim=6)`\n",
      "  app.launch_new_instance()\n",
      "/home/siddharth/miniconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "/home/siddharth/miniconda2/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 690 samples, validate on 230 samples\n",
      "Epoch 1/600\n",
      "0s - loss: 0.1196 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 2/600\n",
      "0s - loss: 0.1193 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 3/600\n",
      "0s - loss: 0.1192 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 4/600\n",
      "0s - loss: 0.1192 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 5/600\n",
      "0s - loss: 0.1192 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 6/600\n",
      "0s - loss: 0.1191 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 7/600\n",
      "0s - loss: 0.1191 - acc: 0.4565 - val_loss: 0.1175 - val_acc: 0.4174\n",
      "Epoch 8/600\n",
      "0s - loss: 0.1191 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 9/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 10/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 11/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1174 - val_acc: 0.4174\n",
      "Epoch 12/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 13/600\n",
      "0s - loss: 0.1190 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 14/600\n",
      "0s - loss: 0.1189 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 15/600\n",
      "0s - loss: 0.1189 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 16/600\n",
      "0s - loss: 0.1189 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 17/600\n",
      "0s - loss: 0.1188 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 18/600\n",
      "0s - loss: 0.1188 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 19/600\n",
      "0s - loss: 0.1187 - acc: 0.4565 - val_loss: 0.1173 - val_acc: 0.4174\n",
      "Epoch 20/600\n",
      "0s - loss: 0.1187 - acc: 0.4565 - val_loss: 0.1171 - val_acc: 0.4174\n",
      "Epoch 21/600\n",
      "0s - loss: 0.1187 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 22/600\n",
      "0s - loss: 0.1186 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 23/600\n",
      "0s - loss: 0.1185 - acc: 0.4565 - val_loss: 0.1172 - val_acc: 0.4174\n",
      "Epoch 24/600\n",
      "0s - loss: 0.1185 - acc: 0.4565 - val_loss: 0.1170 - val_acc: 0.4174\n",
      "Epoch 25/600\n",
      "0s - loss: 0.1184 - acc: 0.4565 - val_loss: 0.1169 - val_acc: 0.4174\n",
      "Epoch 26/600\n",
      "0s - loss: 0.1183 - acc: 0.4565 - val_loss: 0.1169 - val_acc: 0.4174\n",
      "Epoch 27/600\n",
      "0s - loss: 0.1183 - acc: 0.4565 - val_loss: 0.1169 - val_acc: 0.4174\n",
      "Epoch 28/600\n",
      "0s - loss: 0.1182 - acc: 0.4565 - val_loss: 0.1169 - val_acc: 0.4174\n",
      "Epoch 29/600\n",
      "0s - loss: 0.1181 - acc: 0.4565 - val_loss: 0.1169 - val_acc: 0.4174\n",
      "Epoch 30/600\n",
      "0s - loss: 0.1180 - acc: 0.4565 - val_loss: 0.1169 - val_acc: 0.4174\n",
      "Epoch 31/600\n",
      "0s - loss: 0.1179 - acc: 0.4565 - val_loss: 0.1167 - val_acc: 0.4174\n",
      "Epoch 32/600\n",
      "0s - loss: 0.1178 - acc: 0.4565 - val_loss: 0.1166 - val_acc: 0.4174\n",
      "Epoch 33/600\n",
      "0s - loss: 0.1177 - acc: 0.4565 - val_loss: 0.1166 - val_acc: 0.4174\n",
      "Epoch 34/600\n",
      "0s - loss: 0.1176 - acc: 0.4565 - val_loss: 0.1165 - val_acc: 0.4174\n",
      "Epoch 35/600\n",
      "0s - loss: 0.1174 - acc: 0.4565 - val_loss: 0.1164 - val_acc: 0.4174\n",
      "Epoch 36/600\n",
      "0s - loss: 0.1172 - acc: 0.4565 - val_loss: 0.1161 - val_acc: 0.4174\n",
      "Epoch 37/600\n",
      "0s - loss: 0.1171 - acc: 0.4565 - val_loss: 0.1158 - val_acc: 0.4174\n",
      "Epoch 38/600\n",
      "0s - loss: 0.1169 - acc: 0.4565 - val_loss: 0.1157 - val_acc: 0.4174\n",
      "Epoch 39/600\n",
      "0s - loss: 0.1168 - acc: 0.4565 - val_loss: 0.1156 - val_acc: 0.4174\n",
      "Epoch 40/600\n",
      "0s - loss: 0.1166 - acc: 0.4565 - val_loss: 0.1154 - val_acc: 0.4174\n",
      "Epoch 41/600\n",
      "0s - loss: 0.1164 - acc: 0.4565 - val_loss: 0.1151 - val_acc: 0.4174\n",
      "Epoch 42/600\n",
      "0s - loss: 0.1163 - acc: 0.4565 - val_loss: 0.1148 - val_acc: 0.4174\n",
      "Epoch 43/600\n",
      "0s - loss: 0.1161 - acc: 0.4565 - val_loss: 0.1147 - val_acc: 0.4174\n",
      "Epoch 44/600\n",
      "0s - loss: 0.1159 - acc: 0.4565 - val_loss: 0.1144 - val_acc: 0.4174\n",
      "Epoch 45/600\n",
      "0s - loss: 0.1156 - acc: 0.4565 - val_loss: 0.1144 - val_acc: 0.4174\n",
      "Epoch 46/600\n",
      "0s - loss: 0.1154 - acc: 0.4565 - val_loss: 0.1140 - val_acc: 0.4174\n",
      "Epoch 47/600\n",
      "0s - loss: 0.1152 - acc: 0.4565 - val_loss: 0.1139 - val_acc: 0.4174\n",
      "Epoch 48/600\n",
      "0s - loss: 0.1149 - acc: 0.4565 - val_loss: 0.1137 - val_acc: 0.4174\n",
      "Epoch 49/600\n",
      "0s - loss: 0.1147 - acc: 0.4565 - val_loss: 0.1134 - val_acc: 0.4174\n",
      "Epoch 50/600\n",
      "0s - loss: 0.1144 - acc: 0.4565 - val_loss: 0.1132 - val_acc: 0.4174\n",
      "Epoch 51/600\n",
      "0s - loss: 0.1141 - acc: 0.4565 - val_loss: 0.1130 - val_acc: 0.4174\n",
      "Epoch 52/600\n",
      "0s - loss: 0.1138 - acc: 0.4565 - val_loss: 0.1127 - val_acc: 0.4174\n",
      "Epoch 53/600\n",
      "0s - loss: 0.1135 - acc: 0.4565 - val_loss: 0.1123 - val_acc: 0.4174\n",
      "Epoch 54/600\n",
      "0s - loss: 0.1132 - acc: 0.4565 - val_loss: 0.1120 - val_acc: 0.4174\n",
      "Epoch 55/600\n",
      "0s - loss: 0.1128 - acc: 0.4565 - val_loss: 0.1118 - val_acc: 0.4174\n",
      "Epoch 56/600\n",
      "0s - loss: 0.1124 - acc: 0.4565 - val_loss: 0.1115 - val_acc: 0.4174\n",
      "Epoch 57/600\n",
      "0s - loss: 0.1121 - acc: 0.4565 - val_loss: 0.1113 - val_acc: 0.4174\n",
      "Epoch 58/600\n",
      "0s - loss: 0.1117 - acc: 0.4565 - val_loss: 0.1110 - val_acc: 0.4174\n",
      "Epoch 59/600\n",
      "0s - loss: 0.1113 - acc: 0.4565 - val_loss: 0.1106 - val_acc: 0.4174\n",
      "Epoch 60/600\n",
      "0s - loss: 0.1108 - acc: 0.4565 - val_loss: 0.1102 - val_acc: 0.4174\n",
      "Epoch 61/600\n",
      "0s - loss: 0.1104 - acc: 0.4565 - val_loss: 0.1099 - val_acc: 0.4174\n",
      "Epoch 62/600\n",
      "0s - loss: 0.1100 - acc: 0.4565 - val_loss: 0.1094 - val_acc: 0.4217\n",
      "Epoch 63/600\n",
      "0s - loss: 0.1096 - acc: 0.4565 - val_loss: 0.1087 - val_acc: 0.4261\n",
      "Epoch 64/600\n",
      "0s - loss: 0.1091 - acc: 0.4594 - val_loss: 0.1085 - val_acc: 0.4261\n",
      "Epoch 65/600\n",
      "0s - loss: 0.1087 - acc: 0.4594 - val_loss: 0.1080 - val_acc: 0.4304\n",
      "Epoch 66/600\n",
      "0s - loss: 0.1083 - acc: 0.4609 - val_loss: 0.1075 - val_acc: 0.4391\n",
      "Epoch 67/600\n",
      "0s - loss: 0.1078 - acc: 0.4797 - val_loss: 0.1071 - val_acc: 0.4391\n",
      "Epoch 68/600\n",
      "0s - loss: 0.1074 - acc: 0.4855 - val_loss: 0.1067 - val_acc: 0.4391\n",
      "Epoch 69/600\n",
      "0s - loss: 0.1069 - acc: 0.4899 - val_loss: 0.1062 - val_acc: 0.4478\n",
      "Epoch 70/600\n",
      "0s - loss: 0.1065 - acc: 0.4971 - val_loss: 0.1056 - val_acc: 0.4609\n",
      "Epoch 71/600\n",
      "0s - loss: 0.1060 - acc: 0.5159 - val_loss: 0.1053 - val_acc: 0.4565\n",
      "Epoch 72/600\n",
      "0s - loss: 0.1054 - acc: 0.5087 - val_loss: 0.1048 - val_acc: 0.4609\n",
      "Epoch 73/600\n",
      "0s - loss: 0.1049 - acc: 0.5130 - val_loss: 0.1042 - val_acc: 0.4696\n",
      "Epoch 74/600\n",
      "0s - loss: 0.1043 - acc: 0.5246 - val_loss: 0.1037 - val_acc: 0.4739\n",
      "Epoch 75/600\n",
      "0s - loss: 0.1037 - acc: 0.5333 - val_loss: 0.1032 - val_acc: 0.4913\n",
      "Epoch 76/600\n",
      "0s - loss: 0.1032 - acc: 0.5362 - val_loss: 0.1026 - val_acc: 0.4957\n",
      "Epoch 77/600\n",
      "0s - loss: 0.1026 - acc: 0.5420 - val_loss: 0.1021 - val_acc: 0.5087\n",
      "Epoch 78/600\n",
      "0s - loss: 0.1019 - acc: 0.5493 - val_loss: 0.1016 - val_acc: 0.5087\n",
      "Epoch 79/600\n",
      "0s - loss: 0.1013 - acc: 0.5536 - val_loss: 0.1008 - val_acc: 0.5217\n",
      "Epoch 80/600\n",
      "0s - loss: 0.1007 - acc: 0.5725 - val_loss: 0.1001 - val_acc: 0.5391\n",
      "Epoch 81/600\n",
      "0s - loss: 0.1001 - acc: 0.5855 - val_loss: 0.0995 - val_acc: 0.5522\n",
      "Epoch 82/600\n",
      "0s - loss: 0.0994 - acc: 0.6014 - val_loss: 0.0991 - val_acc: 0.5435\n",
      "Epoch 83/600\n",
      "0s - loss: 0.0988 - acc: 0.5942 - val_loss: 0.0986 - val_acc: 0.5435\n",
      "Epoch 84/600\n",
      "0s - loss: 0.0981 - acc: 0.5942 - val_loss: 0.0977 - val_acc: 0.5652\n",
      "Epoch 85/600\n",
      "0s - loss: 0.0975 - acc: 0.6116 - val_loss: 0.0973 - val_acc: 0.5609\n",
      "Epoch 86/600\n",
      "0s - loss: 0.0968 - acc: 0.6087 - val_loss: 0.0965 - val_acc: 0.5783\n",
      "Epoch 87/600\n",
      "0s - loss: 0.0962 - acc: 0.6261 - val_loss: 0.0958 - val_acc: 0.6000\n",
      "Epoch 88/600\n",
      "0s - loss: 0.0955 - acc: 0.6464 - val_loss: 0.0953 - val_acc: 0.6000\n",
      "Epoch 89/600\n",
      "0s - loss: 0.0949 - acc: 0.6449 - val_loss: 0.0945 - val_acc: 0.6217\n",
      "Epoch 90/600\n",
      "0s - loss: 0.0942 - acc: 0.6667 - val_loss: 0.0938 - val_acc: 0.6435\n",
      "Epoch 91/600\n",
      "0s - loss: 0.0935 - acc: 0.6797 - val_loss: 0.0931 - val_acc: 0.6478\n",
      "Epoch 92/600\n",
      "0s - loss: 0.0928 - acc: 0.6899 - val_loss: 0.0926 - val_acc: 0.6435\n",
      "Epoch 93/600\n",
      "0s - loss: 0.0921 - acc: 0.6841 - val_loss: 0.0920 - val_acc: 0.6478\n",
      "Epoch 94/600\n",
      "0s - loss: 0.0914 - acc: 0.6928 - val_loss: 0.0912 - val_acc: 0.6478\n",
      "Epoch 95/600\n",
      "0s - loss: 0.0907 - acc: 0.7058 - val_loss: 0.0906 - val_acc: 0.6522\n",
      "Epoch 96/600\n",
      "0s - loss: 0.0900 - acc: 0.7043 - val_loss: 0.0898 - val_acc: 0.6609\n",
      "Epoch 97/600\n",
      "0s - loss: 0.0893 - acc: 0.7174 - val_loss: 0.0890 - val_acc: 0.6696\n",
      "Epoch 98/600\n",
      "0s - loss: 0.0886 - acc: 0.7261 - val_loss: 0.0883 - val_acc: 0.6696\n",
      "Epoch 99/600\n",
      "0s - loss: 0.0879 - acc: 0.7333 - val_loss: 0.0878 - val_acc: 0.6696\n",
      "Epoch 100/600\n",
      "0s - loss: 0.0872 - acc: 0.7304 - val_loss: 0.0871 - val_acc: 0.6696\n",
      "Epoch 101/600\n",
      "0s - loss: 0.0865 - acc: 0.7420 - val_loss: 0.0865 - val_acc: 0.6696\n",
      "Epoch 102/600\n",
      "0s - loss: 0.0858 - acc: 0.7406 - val_loss: 0.0859 - val_acc: 0.6696\n",
      "Epoch 103/600\n",
      "0s - loss: 0.0851 - acc: 0.7449 - val_loss: 0.0851 - val_acc: 0.6783\n",
      "Epoch 104/600\n",
      "0s - loss: 0.0844 - acc: 0.7536 - val_loss: 0.0843 - val_acc: 0.6913\n",
      "Epoch 105/600\n",
      "0s - loss: 0.0837 - acc: 0.7623 - val_loss: 0.0836 - val_acc: 0.7087\n",
      "Epoch 106/600\n",
      "0s - loss: 0.0830 - acc: 0.7681 - val_loss: 0.0827 - val_acc: 0.7348\n",
      "Epoch 107/600\n",
      "0s - loss: 0.0823 - acc: 0.7768 - val_loss: 0.0821 - val_acc: 0.7348\n",
      "Epoch 108/600\n",
      "0s - loss: 0.0816 - acc: 0.7783 - val_loss: 0.0813 - val_acc: 0.7478\n",
      "Epoch 109/600\n",
      "0s - loss: 0.0809 - acc: 0.7841 - val_loss: 0.0807 - val_acc: 0.7478\n",
      "Epoch 110/600\n",
      "0s - loss: 0.0802 - acc: 0.7855 - val_loss: 0.0801 - val_acc: 0.7478\n",
      "Epoch 111/600\n",
      "0s - loss: 0.0796 - acc: 0.7812 - val_loss: 0.0792 - val_acc: 0.7609\n",
      "Epoch 112/600\n",
      "0s - loss: 0.0789 - acc: 0.7899 - val_loss: 0.0784 - val_acc: 0.7652\n",
      "Epoch 113/600\n",
      "0s - loss: 0.0782 - acc: 0.8029 - val_loss: 0.0779 - val_acc: 0.7652\n",
      "Epoch 114/600\n",
      "0s - loss: 0.0775 - acc: 0.8029 - val_loss: 0.0772 - val_acc: 0.7652\n",
      "Epoch 115/600\n",
      "0s - loss: 0.0769 - acc: 0.8116 - val_loss: 0.0767 - val_acc: 0.7652\n",
      "Epoch 116/600\n",
      "0s - loss: 0.0762 - acc: 0.8014 - val_loss: 0.0759 - val_acc: 0.7696\n",
      "Epoch 117/600\n",
      "0s - loss: 0.0756 - acc: 0.8145 - val_loss: 0.0753 - val_acc: 0.7739\n",
      "Epoch 118/600\n",
      "0s - loss: 0.0749 - acc: 0.8174 - val_loss: 0.0746 - val_acc: 0.7783\n",
      "Epoch 119/600\n",
      "0s - loss: 0.0743 - acc: 0.8217 - val_loss: 0.0738 - val_acc: 0.7826\n",
      "Epoch 120/600\n",
      "0s - loss: 0.0736 - acc: 0.8246 - val_loss: 0.0732 - val_acc: 0.7826\n",
      "Epoch 121/600\n",
      "0s - loss: 0.0730 - acc: 0.8275 - val_loss: 0.0724 - val_acc: 0.7957\n",
      "Epoch 122/600\n",
      "0s - loss: 0.0724 - acc: 0.8319 - val_loss: 0.0721 - val_acc: 0.7826\n",
      "Epoch 123/600\n",
      "0s - loss: 0.0718 - acc: 0.8304 - val_loss: 0.0712 - val_acc: 0.8043\n",
      "Epoch 124/600\n",
      "0s - loss: 0.0712 - acc: 0.8333 - val_loss: 0.0704 - val_acc: 0.8130\n",
      "Epoch 125/600\n",
      "0s - loss: 0.0707 - acc: 0.8406 - val_loss: 0.0700 - val_acc: 0.8130\n",
      "Epoch 126/600\n",
      "0s - loss: 0.0701 - acc: 0.8362 - val_loss: 0.0694 - val_acc: 0.8174\n",
      "Epoch 127/600\n",
      "0s - loss: 0.0695 - acc: 0.8391 - val_loss: 0.0687 - val_acc: 0.8174\n",
      "Epoch 128/600\n",
      "0s - loss: 0.0689 - acc: 0.8406 - val_loss: 0.0681 - val_acc: 0.8174\n",
      "Epoch 129/600\n",
      "0s - loss: 0.0684 - acc: 0.8406 - val_loss: 0.0676 - val_acc: 0.8174\n",
      "Epoch 130/600\n",
      "0s - loss: 0.0678 - acc: 0.8406 - val_loss: 0.0672 - val_acc: 0.8174\n",
      "Epoch 131/600\n",
      "0s - loss: 0.0673 - acc: 0.8377 - val_loss: 0.0666 - val_acc: 0.8217\n",
      "Epoch 132/600\n",
      "0s - loss: 0.0667 - acc: 0.8435 - val_loss: 0.0658 - val_acc: 0.8261\n",
      "Epoch 133/600\n",
      "0s - loss: 0.0662 - acc: 0.8478 - val_loss: 0.0654 - val_acc: 0.8261\n",
      "Epoch 134/600\n",
      "0s - loss: 0.0656 - acc: 0.8449 - val_loss: 0.0648 - val_acc: 0.8261\n",
      "Epoch 135/600\n",
      "0s - loss: 0.0651 - acc: 0.8478 - val_loss: 0.0640 - val_acc: 0.8261\n",
      "Epoch 136/600\n",
      "0s - loss: 0.0645 - acc: 0.8478 - val_loss: 0.0635 - val_acc: 0.8261\n",
      "Epoch 137/600\n",
      "0s - loss: 0.0640 - acc: 0.8522 - val_loss: 0.0633 - val_acc: 0.8261\n",
      "Epoch 138/600\n",
      "0s - loss: 0.0635 - acc: 0.8493 - val_loss: 0.0628 - val_acc: 0.8261\n",
      "Epoch 139/600\n",
      "0s - loss: 0.0630 - acc: 0.8507 - val_loss: 0.0620 - val_acc: 0.8261\n",
      "Epoch 140/600\n",
      "0s - loss: 0.0625 - acc: 0.8565 - val_loss: 0.0615 - val_acc: 0.8304\n",
      "Epoch 141/600\n",
      "0s - loss: 0.0620 - acc: 0.8565 - val_loss: 0.0609 - val_acc: 0.8217\n",
      "Epoch 142/600\n",
      "0s - loss: 0.0615 - acc: 0.8594 - val_loss: 0.0603 - val_acc: 0.8217\n",
      "Epoch 143/600\n",
      "0s - loss: 0.0610 - acc: 0.8638 - val_loss: 0.0599 - val_acc: 0.8217\n",
      "Epoch 144/600\n",
      "0s - loss: 0.0605 - acc: 0.8638 - val_loss: 0.0593 - val_acc: 0.8261\n",
      "Epoch 145/600\n",
      "0s - loss: 0.0600 - acc: 0.8638 - val_loss: 0.0586 - val_acc: 0.8261\n",
      "Epoch 146/600\n",
      "0s - loss: 0.0596 - acc: 0.8681 - val_loss: 0.0584 - val_acc: 0.8261\n",
      "Epoch 147/600\n",
      "0s - loss: 0.0591 - acc: 0.8652 - val_loss: 0.0576 - val_acc: 0.8261\n",
      "Epoch 148/600\n",
      "0s - loss: 0.0587 - acc: 0.8681 - val_loss: 0.0574 - val_acc: 0.8261\n",
      "Epoch 149/600\n",
      "0s - loss: 0.0582 - acc: 0.8681 - val_loss: 0.0568 - val_acc: 0.8304\n",
      "Epoch 150/600\n",
      "0s - loss: 0.0578 - acc: 0.8696 - val_loss: 0.0565 - val_acc: 0.8304\n",
      "Epoch 151/600\n",
      "0s - loss: 0.0574 - acc: 0.8710 - val_loss: 0.0560 - val_acc: 0.8304\n",
      "Epoch 152/600\n",
      "0s - loss: 0.0569 - acc: 0.8696 - val_loss: 0.0553 - val_acc: 0.8304\n",
      "Epoch 153/600\n",
      "0s - loss: 0.0565 - acc: 0.8739 - val_loss: 0.0548 - val_acc: 0.8304\n",
      "Epoch 154/600\n",
      "0s - loss: 0.0561 - acc: 0.8739 - val_loss: 0.0544 - val_acc: 0.8304\n",
      "Epoch 155/600\n",
      "0s - loss: 0.0557 - acc: 0.8754 - val_loss: 0.0542 - val_acc: 0.8304\n",
      "Epoch 156/600\n",
      "0s - loss: 0.0553 - acc: 0.8768 - val_loss: 0.0536 - val_acc: 0.8304\n",
      "Epoch 157/600\n",
      "0s - loss: 0.0549 - acc: 0.8754 - val_loss: 0.0533 - val_acc: 0.8304\n",
      "Epoch 158/600\n",
      "0s - loss: 0.0546 - acc: 0.8768 - val_loss: 0.0526 - val_acc: 0.8348\n",
      "Epoch 159/600\n",
      "0s - loss: 0.0542 - acc: 0.8768 - val_loss: 0.0524 - val_acc: 0.8304\n",
      "Epoch 160/600\n",
      "0s - loss: 0.0539 - acc: 0.8783 - val_loss: 0.0524 - val_acc: 0.8304\n",
      "Epoch 161/600\n",
      "0s - loss: 0.0535 - acc: 0.8754 - val_loss: 0.0519 - val_acc: 0.8348\n",
      "Epoch 162/600\n",
      "0s - loss: 0.0532 - acc: 0.8754 - val_loss: 0.0517 - val_acc: 0.8304\n",
      "Epoch 163/600\n",
      "0s - loss: 0.0528 - acc: 0.8754 - val_loss: 0.0511 - val_acc: 0.8348\n",
      "Epoch 164/600\n",
      "0s - loss: 0.0525 - acc: 0.8783 - val_loss: 0.0506 - val_acc: 0.8391\n",
      "Epoch 165/600\n",
      "0s - loss: 0.0521 - acc: 0.8768 - val_loss: 0.0502 - val_acc: 0.8391\n",
      "Epoch 166/600\n",
      "0s - loss: 0.0518 - acc: 0.8783 - val_loss: 0.0497 - val_acc: 0.8391\n",
      "Epoch 167/600\n",
      "0s - loss: 0.0515 - acc: 0.8826 - val_loss: 0.0492 - val_acc: 0.8522\n",
      "Epoch 168/600\n",
      "0s - loss: 0.0512 - acc: 0.8841 - val_loss: 0.0490 - val_acc: 0.8522\n",
      "Epoch 169/600\n",
      "0s - loss: 0.0509 - acc: 0.8855 - val_loss: 0.0488 - val_acc: 0.8435\n",
      "Epoch 170/600\n",
      "0s - loss: 0.0506 - acc: 0.8841 - val_loss: 0.0484 - val_acc: 0.8478\n",
      "Epoch 171/600\n",
      "0s - loss: 0.0503 - acc: 0.8870 - val_loss: 0.0481 - val_acc: 0.8522\n",
      "Epoch 172/600\n",
      "0s - loss: 0.0500 - acc: 0.8884 - val_loss: 0.0481 - val_acc: 0.8391\n",
      "Epoch 173/600\n",
      "0s - loss: 0.0497 - acc: 0.8884 - val_loss: 0.0476 - val_acc: 0.8522\n",
      "Epoch 174/600\n",
      "0s - loss: 0.0494 - acc: 0.8884 - val_loss: 0.0471 - val_acc: 0.8565\n",
      "Epoch 175/600\n",
      "0s - loss: 0.0491 - acc: 0.8913 - val_loss: 0.0468 - val_acc: 0.8652\n",
      "Epoch 176/600\n",
      "0s - loss: 0.0489 - acc: 0.8899 - val_loss: 0.0466 - val_acc: 0.8652\n",
      "Epoch 177/600\n",
      "0s - loss: 0.0487 - acc: 0.8899 - val_loss: 0.0461 - val_acc: 0.8696\n",
      "Epoch 178/600\n",
      "0s - loss: 0.0484 - acc: 0.8928 - val_loss: 0.0459 - val_acc: 0.8696\n",
      "Epoch 179/600\n",
      "0s - loss: 0.0482 - acc: 0.8928 - val_loss: 0.0457 - val_acc: 0.8696\n",
      "Epoch 180/600\n",
      "0s - loss: 0.0479 - acc: 0.8942 - val_loss: 0.0456 - val_acc: 0.8696\n",
      "Epoch 181/600\n",
      "0s - loss: 0.0477 - acc: 0.8942 - val_loss: 0.0452 - val_acc: 0.8696\n",
      "Epoch 182/600\n",
      "0s - loss: 0.0474 - acc: 0.8942 - val_loss: 0.0451 - val_acc: 0.8696\n",
      "Epoch 183/600\n",
      "0s - loss: 0.0472 - acc: 0.8942 - val_loss: 0.0447 - val_acc: 0.8652\n",
      "Epoch 184/600\n",
      "0s - loss: 0.0469 - acc: 0.8942 - val_loss: 0.0444 - val_acc: 0.8652\n",
      "Epoch 185/600\n",
      "0s - loss: 0.0467 - acc: 0.8913 - val_loss: 0.0444 - val_acc: 0.8652\n",
      "Epoch 186/600\n",
      "0s - loss: 0.0465 - acc: 0.8928 - val_loss: 0.0443 - val_acc: 0.8652\n",
      "Epoch 187/600\n",
      "0s - loss: 0.0463 - acc: 0.8942 - val_loss: 0.0439 - val_acc: 0.8652\n",
      "Epoch 188/600\n",
      "0s - loss: 0.0461 - acc: 0.8899 - val_loss: 0.0437 - val_acc: 0.8652\n",
      "Epoch 189/600\n",
      "0s - loss: 0.0459 - acc: 0.8913 - val_loss: 0.0435 - val_acc: 0.8652\n",
      "Epoch 190/600\n",
      "0s - loss: 0.0457 - acc: 0.8899 - val_loss: 0.0432 - val_acc: 0.8652\n",
      "Epoch 191/600\n",
      "0s - loss: 0.0455 - acc: 0.8899 - val_loss: 0.0430 - val_acc: 0.8652\n",
      "Epoch 192/600\n",
      "0s - loss: 0.0454 - acc: 0.8899 - val_loss: 0.0431 - val_acc: 0.8696\n",
      "Epoch 193/600\n",
      "0s - loss: 0.0452 - acc: 0.8870 - val_loss: 0.0426 - val_acc: 0.8783\n",
      "Epoch 194/600\n",
      "0s - loss: 0.0450 - acc: 0.8870 - val_loss: 0.0422 - val_acc: 0.8783\n",
      "Epoch 195/600\n",
      "0s - loss: 0.0449 - acc: 0.8899 - val_loss: 0.0424 - val_acc: 0.8870\n",
      "Epoch 196/600\n",
      "0s - loss: 0.0447 - acc: 0.8870 - val_loss: 0.0422 - val_acc: 0.8826\n",
      "Epoch 197/600\n",
      "0s - loss: 0.0446 - acc: 0.8884 - val_loss: 0.0418 - val_acc: 0.8783\n",
      "Epoch 198/600\n",
      "0s - loss: 0.0444 - acc: 0.8870 - val_loss: 0.0414 - val_acc: 0.8783\n",
      "Epoch 199/600\n",
      "0s - loss: 0.0443 - acc: 0.8884 - val_loss: 0.0417 - val_acc: 0.8913\n",
      "Epoch 200/600\n",
      "0s - loss: 0.0442 - acc: 0.8855 - val_loss: 0.0412 - val_acc: 0.8783\n",
      "Epoch 201/600\n",
      "0s - loss: 0.0440 - acc: 0.8855 - val_loss: 0.0413 - val_acc: 0.8870\n",
      "Epoch 202/600\n",
      "0s - loss: 0.0439 - acc: 0.8855 - val_loss: 0.0413 - val_acc: 0.8870\n",
      "Epoch 203/600\n",
      "0s - loss: 0.0438 - acc: 0.8841 - val_loss: 0.0411 - val_acc: 0.8870\n",
      "Epoch 204/600\n",
      "0s - loss: 0.0437 - acc: 0.8841 - val_loss: 0.0407 - val_acc: 0.8739\n",
      "Epoch 205/600\n",
      "0s - loss: 0.0436 - acc: 0.8812 - val_loss: 0.0408 - val_acc: 0.8870\n",
      "Epoch 206/600\n",
      "0s - loss: 0.0435 - acc: 0.8826 - val_loss: 0.0406 - val_acc: 0.8783\n",
      "Epoch 207/600\n",
      "0s - loss: 0.0434 - acc: 0.8826 - val_loss: 0.0402 - val_acc: 0.8739\n",
      "Epoch 208/600\n",
      "0s - loss: 0.0433 - acc: 0.8812 - val_loss: 0.0399 - val_acc: 0.8696\n",
      "Epoch 209/600\n",
      "0s - loss: 0.0432 - acc: 0.8826 - val_loss: 0.0400 - val_acc: 0.8783\n",
      "Epoch 210/600\n",
      "0s - loss: 0.0431 - acc: 0.8797 - val_loss: 0.0403 - val_acc: 0.8783\n",
      "Epoch 211/600\n",
      "0s - loss: 0.0430 - acc: 0.8797 - val_loss: 0.0402 - val_acc: 0.8783\n",
      "Epoch 212/600\n",
      "0s - loss: 0.0429 - acc: 0.8797 - val_loss: 0.0397 - val_acc: 0.8783\n",
      "Epoch 213/600\n",
      "0s - loss: 0.0429 - acc: 0.8812 - val_loss: 0.0394 - val_acc: 0.8739\n",
      "Epoch 214/600\n",
      "0s - loss: 0.0428 - acc: 0.8783 - val_loss: 0.0394 - val_acc: 0.8739\n",
      "Epoch 215/600\n",
      "0s - loss: 0.0428 - acc: 0.8783 - val_loss: 0.0392 - val_acc: 0.8739\n",
      "Epoch 216/600\n",
      "0s - loss: 0.0427 - acc: 0.8797 - val_loss: 0.0390 - val_acc: 0.8739\n",
      "Epoch 217/600\n",
      "0s - loss: 0.0426 - acc: 0.8826 - val_loss: 0.0391 - val_acc: 0.8783\n",
      "Epoch 218/600\n",
      "0s - loss: 0.0425 - acc: 0.8812 - val_loss: 0.0392 - val_acc: 0.8783\n",
      "Epoch 219/600\n",
      "0s - loss: 0.0424 - acc: 0.8797 - val_loss: 0.0392 - val_acc: 0.8783\n",
      "Epoch 220/600\n",
      "0s - loss: 0.0424 - acc: 0.8783 - val_loss: 0.0394 - val_acc: 0.8783\n",
      "Epoch 221/600\n",
      "0s - loss: 0.0423 - acc: 0.8783 - val_loss: 0.0393 - val_acc: 0.8783\n",
      "Epoch 222/600\n",
      "0s - loss: 0.0423 - acc: 0.8768 - val_loss: 0.0394 - val_acc: 0.8783\n",
      "Epoch 223/600\n",
      "0s - loss: 0.0422 - acc: 0.8812 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 224/600\n",
      "0s - loss: 0.0421 - acc: 0.8826 - val_loss: 0.0387 - val_acc: 0.8783\n",
      "Epoch 225/600\n",
      "0s - loss: 0.0421 - acc: 0.8812 - val_loss: 0.0385 - val_acc: 0.8783\n",
      "Epoch 226/600\n",
      "0s - loss: 0.0421 - acc: 0.8870 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 227/600\n",
      "0s - loss: 0.0420 - acc: 0.8870 - val_loss: 0.0387 - val_acc: 0.8783\n",
      "Epoch 228/600\n",
      "0s - loss: 0.0419 - acc: 0.8826 - val_loss: 0.0387 - val_acc: 0.8783\n",
      "Epoch 229/600\n",
      "0s - loss: 0.0419 - acc: 0.8826 - val_loss: 0.0388 - val_acc: 0.8783\n",
      "Epoch 230/600\n",
      "0s - loss: 0.0420 - acc: 0.8797 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 231/600\n",
      "0s - loss: 0.0418 - acc: 0.8870 - val_loss: 0.0385 - val_acc: 0.8783\n",
      "Epoch 232/600\n",
      "0s - loss: 0.0417 - acc: 0.8855 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 233/600\n",
      "0s - loss: 0.0417 - acc: 0.8855 - val_loss: 0.0383 - val_acc: 0.8783\n",
      "Epoch 234/600\n",
      "0s - loss: 0.0417 - acc: 0.8855 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 235/600\n",
      "0s - loss: 0.0416 - acc: 0.8855 - val_loss: 0.0381 - val_acc: 0.8783\n",
      "Epoch 236/600\n",
      "0s - loss: 0.0416 - acc: 0.8855 - val_loss: 0.0382 - val_acc: 0.8783\n",
      "Epoch 237/600\n",
      "0s - loss: 0.0416 - acc: 0.8855 - val_loss: 0.0385 - val_acc: 0.8783\n",
      "Epoch 238/600\n",
      "0s - loss: 0.0415 - acc: 0.8826 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 239/600\n",
      "0s - loss: 0.0415 - acc: 0.8870 - val_loss: 0.0383 - val_acc: 0.8783\n",
      "Epoch 240/600\n",
      "0s - loss: 0.0414 - acc: 0.8884 - val_loss: 0.0382 - val_acc: 0.8783\n",
      "Epoch 241/600\n",
      "0s - loss: 0.0414 - acc: 0.8884 - val_loss: 0.0381 - val_acc: 0.8783\n",
      "Epoch 242/600\n",
      "0s - loss: 0.0414 - acc: 0.8855 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 243/600\n",
      "0s - loss: 0.0414 - acc: 0.8870 - val_loss: 0.0385 - val_acc: 0.8783\n",
      "Epoch 244/600\n",
      "0s - loss: 0.0413 - acc: 0.8841 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 245/600\n",
      "0s - loss: 0.0413 - acc: 0.8855 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 246/600\n",
      "0s - loss: 0.0413 - acc: 0.8855 - val_loss: 0.0378 - val_acc: 0.8826\n",
      "Epoch 247/600\n",
      "0s - loss: 0.0412 - acc: 0.8826 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 248/600\n",
      "0s - loss: 0.0412 - acc: 0.8870 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 249/600\n",
      "0s - loss: 0.0412 - acc: 0.8826 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 250/600\n",
      "0s - loss: 0.0411 - acc: 0.8855 - val_loss: 0.0379 - val_acc: 0.8826\n",
      "Epoch 251/600\n",
      "0s - loss: 0.0411 - acc: 0.8855 - val_loss: 0.0381 - val_acc: 0.8783\n",
      "Epoch 252/600\n",
      "0s - loss: 0.0411 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8826\n",
      "Epoch 253/600\n",
      "0s - loss: 0.0410 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 254/600\n",
      "0s - loss: 0.0410 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 255/600\n",
      "0s - loss: 0.0411 - acc: 0.8870 - val_loss: 0.0376 - val_acc: 0.8870\n",
      "Epoch 256/600\n",
      "0s - loss: 0.0410 - acc: 0.8826 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 257/600\n",
      "0s - loss: 0.0410 - acc: 0.8841 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 258/600\n",
      "0s - loss: 0.0409 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 259/600\n",
      "0s - loss: 0.0409 - acc: 0.8841 - val_loss: 0.0381 - val_acc: 0.8783\n",
      "Epoch 260/600\n",
      "0s - loss: 0.0409 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 261/600\n",
      "0s - loss: 0.0409 - acc: 0.8870 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 262/600\n",
      "0s - loss: 0.0408 - acc: 0.8855 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 263/600\n",
      "0s - loss: 0.0408 - acc: 0.8870 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 264/600\n",
      "0s - loss: 0.0408 - acc: 0.8841 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 265/600\n",
      "0s - loss: 0.0408 - acc: 0.8855 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 266/600\n",
      "0s - loss: 0.0407 - acc: 0.8855 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 267/600\n",
      "0s - loss: 0.0407 - acc: 0.8855 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 268/600\n",
      "0s - loss: 0.0407 - acc: 0.8855 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 269/600\n",
      "0s - loss: 0.0407 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 270/600\n",
      "0s - loss: 0.0407 - acc: 0.8870 - val_loss: 0.0375 - val_acc: 0.8826\n",
      "Epoch 271/600\n",
      "0s - loss: 0.0406 - acc: 0.8855 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 272/600\n",
      "0s - loss: 0.0406 - acc: 0.8841 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 273/600\n",
      "0s - loss: 0.0406 - acc: 0.8855 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 274/600\n",
      "0s - loss: 0.0406 - acc: 0.8855 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 275/600\n",
      "0s - loss: 0.0406 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 276/600\n",
      "0s - loss: 0.0405 - acc: 0.8855 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 277/600\n",
      "0s - loss: 0.0405 - acc: 0.8884 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 278/600\n",
      "0s - loss: 0.0406 - acc: 0.8855 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 279/600\n",
      "0s - loss: 0.0406 - acc: 0.8855 - val_loss: 0.0374 - val_acc: 0.8826\n",
      "Epoch 280/600\n",
      "0s - loss: 0.0405 - acc: 0.8870 - val_loss: 0.0374 - val_acc: 0.8826\n",
      "Epoch 281/600\n",
      "0s - loss: 0.0406 - acc: 0.8870 - val_loss: 0.0373 - val_acc: 0.8913\n",
      "Epoch 282/600\n",
      "0s - loss: 0.0405 - acc: 0.8899 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 283/600\n",
      "0s - loss: 0.0405 - acc: 0.8884 - val_loss: 0.0375 - val_acc: 0.8826\n",
      "Epoch 284/600\n",
      "0s - loss: 0.0404 - acc: 0.8870 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 285/600\n",
      "0s - loss: 0.0405 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 286/600\n",
      "0s - loss: 0.0404 - acc: 0.8855 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 287/600\n",
      "0s - loss: 0.0404 - acc: 0.8855 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 288/600\n",
      "0s - loss: 0.0404 - acc: 0.8884 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 289/600\n",
      "0s - loss: 0.0404 - acc: 0.8870 - val_loss: 0.0375 - val_acc: 0.8826\n",
      "Epoch 290/600\n",
      "0s - loss: 0.0404 - acc: 0.8870 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 291/600\n",
      "0s - loss: 0.0403 - acc: 0.8870 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 292/600\n",
      "0s - loss: 0.0403 - acc: 0.8884 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 293/600\n",
      "0s - loss: 0.0404 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 294/600\n",
      "0s - loss: 0.0403 - acc: 0.8855 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 295/600\n",
      "0s - loss: 0.0403 - acc: 0.8870 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 296/600\n",
      "0s - loss: 0.0403 - acc: 0.8884 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 297/600\n",
      "0s - loss: 0.0403 - acc: 0.8884 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 298/600\n",
      "0s - loss: 0.0403 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 299/600\n",
      "0s - loss: 0.0403 - acc: 0.8870 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 300/600\n",
      "0s - loss: 0.0402 - acc: 0.8855 - val_loss: 0.0375 - val_acc: 0.8870\n",
      "Epoch 301/600\n",
      "0s - loss: 0.0402 - acc: 0.8899 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 302/600\n",
      "0s - loss: 0.0402 - acc: 0.8870 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 303/600\n",
      "0s - loss: 0.0402 - acc: 0.8841 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 304/600\n",
      "0s - loss: 0.0402 - acc: 0.8870 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 305/600\n",
      "0s - loss: 0.0402 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 306/600\n",
      "0s - loss: 0.0402 - acc: 0.8855 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 307/600\n",
      "0s - loss: 0.0402 - acc: 0.8870 - val_loss: 0.0377 - val_acc: 0.8783\n",
      "Epoch 308/600\n",
      "0s - loss: 0.0403 - acc: 0.8884 - val_loss: 0.0381 - val_acc: 0.8739\n",
      "Epoch 309/600\n",
      "0s - loss: 0.0402 - acc: 0.8841 - val_loss: 0.0381 - val_acc: 0.8739\n",
      "Epoch 310/600\n",
      "0s - loss: 0.0402 - acc: 0.8826 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 311/600\n",
      "0s - loss: 0.0401 - acc: 0.8870 - val_loss: 0.0377 - val_acc: 0.8783\n",
      "Epoch 312/600\n",
      "0s - loss: 0.0401 - acc: 0.8884 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 313/600\n",
      "0s - loss: 0.0401 - acc: 0.8870 - val_loss: 0.0377 - val_acc: 0.8783\n",
      "Epoch 314/600\n",
      "0s - loss: 0.0401 - acc: 0.8870 - val_loss: 0.0375 - val_acc: 0.8870\n",
      "Epoch 315/600\n",
      "0s - loss: 0.0401 - acc: 0.8913 - val_loss: 0.0375 - val_acc: 0.8870\n",
      "Epoch 316/600\n",
      "0s - loss: 0.0401 - acc: 0.8913 - val_loss: 0.0377 - val_acc: 0.8783\n",
      "Epoch 317/600\n",
      "0s - loss: 0.0401 - acc: 0.8899 - val_loss: 0.0375 - val_acc: 0.8870\n",
      "Epoch 318/600\n",
      "0s - loss: 0.0401 - acc: 0.8913 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 319/600\n",
      "0s - loss: 0.0401 - acc: 0.8913 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 320/600\n",
      "0s - loss: 0.0401 - acc: 0.8870 - val_loss: 0.0381 - val_acc: 0.8739\n",
      "Epoch 321/600\n",
      "0s - loss: 0.0401 - acc: 0.8812 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 322/600\n",
      "0s - loss: 0.0400 - acc: 0.8913 - val_loss: 0.0380 - val_acc: 0.8739\n",
      "Epoch 323/600\n",
      "0s - loss: 0.0401 - acc: 0.8870 - val_loss: 0.0382 - val_acc: 0.8783\n",
      "Epoch 324/600\n",
      "0s - loss: 0.0400 - acc: 0.8826 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 325/600\n",
      "0s - loss: 0.0400 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8739\n",
      "Epoch 326/600\n",
      "0s - loss: 0.0400 - acc: 0.8855 - val_loss: 0.0380 - val_acc: 0.8739\n",
      "Epoch 327/600\n",
      "0s - loss: 0.0400 - acc: 0.8841 - val_loss: 0.0383 - val_acc: 0.8783\n",
      "Epoch 328/600\n",
      "0s - loss: 0.0400 - acc: 0.8812 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 329/600\n",
      "0s - loss: 0.0400 - acc: 0.8855 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 330/600\n",
      "0s - loss: 0.0400 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 331/600\n",
      "0s - loss: 0.0400 - acc: 0.8884 - val_loss: 0.0381 - val_acc: 0.8783\n",
      "Epoch 332/600\n",
      "0s - loss: 0.0401 - acc: 0.8812 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 333/600\n",
      "0s - loss: 0.0400 - acc: 0.8928 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 334/600\n",
      "0s - loss: 0.0400 - acc: 0.8855 - val_loss: 0.0381 - val_acc: 0.8783\n",
      "Epoch 335/600\n",
      "0s - loss: 0.0400 - acc: 0.8826 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 336/600\n",
      "0s - loss: 0.0400 - acc: 0.8826 - val_loss: 0.0382 - val_acc: 0.8783\n",
      "Epoch 337/600\n",
      "0s - loss: 0.0400 - acc: 0.8812 - val_loss: 0.0383 - val_acc: 0.8783\n",
      "Epoch 338/600\n",
      "0s - loss: 0.0399 - acc: 0.8841 - val_loss: 0.0382 - val_acc: 0.8783\n",
      "Epoch 339/600\n",
      "0s - loss: 0.0399 - acc: 0.8812 - val_loss: 0.0382 - val_acc: 0.8783\n",
      "Epoch 340/600\n",
      "0s - loss: 0.0400 - acc: 0.8812 - val_loss: 0.0384 - val_acc: 0.8783\n",
      "Epoch 341/600\n",
      "0s - loss: 0.0399 - acc: 0.8797 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 342/600\n",
      "0s - loss: 0.0399 - acc: 0.8855 - val_loss: 0.0383 - val_acc: 0.8783\n",
      "Epoch 343/600\n",
      "0s - loss: 0.0399 - acc: 0.8797 - val_loss: 0.0381 - val_acc: 0.8783\n",
      "Epoch 344/600\n",
      "0s - loss: 0.0399 - acc: 0.8826 - val_loss: 0.0378 - val_acc: 0.8826\n",
      "Epoch 345/600\n",
      "0s - loss: 0.0399 - acc: 0.8913 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 346/600\n",
      "0s - loss: 0.0399 - acc: 0.8841 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 347/600\n",
      "0s - loss: 0.0400 - acc: 0.8841 - val_loss: 0.0385 - val_acc: 0.8783\n",
      "Epoch 348/600\n",
      "0s - loss: 0.0399 - acc: 0.8783 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 349/600\n",
      "0s - loss: 0.0398 - acc: 0.8841 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 350/600\n",
      "0s - loss: 0.0398 - acc: 0.8855 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 351/600\n",
      "0s - loss: 0.0398 - acc: 0.8884 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 352/600\n",
      "0s - loss: 0.0398 - acc: 0.8913 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 353/600\n",
      "0s - loss: 0.0398 - acc: 0.8913 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 354/600\n",
      "0s - loss: 0.0398 - acc: 0.8913 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 355/600\n",
      "0s - loss: 0.0398 - acc: 0.8913 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 356/600\n",
      "0s - loss: 0.0398 - acc: 0.8870 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 357/600\n",
      "0s - loss: 0.0398 - acc: 0.8855 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 358/600\n",
      "0s - loss: 0.0398 - acc: 0.8855 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 359/600\n",
      "0s - loss: 0.0398 - acc: 0.8841 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 360/600\n",
      "0s - loss: 0.0398 - acc: 0.8841 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 361/600\n",
      "0s - loss: 0.0398 - acc: 0.8884 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 362/600\n",
      "0s - loss: 0.0398 - acc: 0.8928 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 363/600\n",
      "0s - loss: 0.0398 - acc: 0.8942 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 364/600\n",
      "0s - loss: 0.0397 - acc: 0.8913 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 365/600\n",
      "0s - loss: 0.0398 - acc: 0.8913 - val_loss: 0.0376 - val_acc: 0.8826\n",
      "Epoch 366/600\n",
      "0s - loss: 0.0398 - acc: 0.8942 - val_loss: 0.0376 - val_acc: 0.8870\n",
      "Epoch 367/600\n",
      "0s - loss: 0.0398 - acc: 0.8942 - val_loss: 0.0375 - val_acc: 0.8870\n",
      "Epoch 368/600\n",
      "0s - loss: 0.0398 - acc: 0.8942 - val_loss: 0.0375 - val_acc: 0.8870\n",
      "Epoch 369/600\n",
      "0s - loss: 0.0398 - acc: 0.8942 - val_loss: 0.0377 - val_acc: 0.8826\n",
      "Epoch 370/600\n",
      "0s - loss: 0.0397 - acc: 0.8913 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 371/600\n",
      "0s - loss: 0.0397 - acc: 0.8913 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 372/600\n",
      "0s - loss: 0.0397 - acc: 0.8870 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 373/600\n",
      "0s - loss: 0.0397 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8783\n",
      "Epoch 374/600\n",
      "0s - loss: 0.0397 - acc: 0.8899 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 375/600\n",
      "0s - loss: 0.0397 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8826\n",
      "Epoch 376/600\n",
      "0s - loss: 0.0398 - acc: 0.8899 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 377/600\n",
      "0s - loss: 0.0397 - acc: 0.8826 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 378/600\n",
      "0s - loss: 0.0397 - acc: 0.8884 - val_loss: 0.0377 - val_acc: 0.8783\n",
      "Epoch 379/600\n",
      "0s - loss: 0.0397 - acc: 0.8928 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 380/600\n",
      "0s - loss: 0.0397 - acc: 0.8928 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 381/600\n",
      "0s - loss: 0.0397 - acc: 0.8855 - val_loss: 0.0378 - val_acc: 0.8783\n",
      "Epoch 382/600\n",
      "0s - loss: 0.0397 - acc: 0.8913 - val_loss: 0.0377 - val_acc: 0.8783\n",
      "Epoch 383/600\n",
      "0s - loss: 0.0397 - acc: 0.8942 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 384/600\n",
      "0s - loss: 0.0397 - acc: 0.8913 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 385/600\n",
      "0s - loss: 0.0396 - acc: 0.8870 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 386/600\n",
      "0s - loss: 0.0397 - acc: 0.8870 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 387/600\n",
      "0s - loss: 0.0396 - acc: 0.8841 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 388/600\n",
      "0s - loss: 0.0396 - acc: 0.8855 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 389/600\n",
      "0s - loss: 0.0397 - acc: 0.8870 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 390/600\n",
      "0s - loss: 0.0396 - acc: 0.8841 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 391/600\n",
      "0s - loss: 0.0397 - acc: 0.8870 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 392/600\n",
      "0s - loss: 0.0396 - acc: 0.8841 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 393/600\n",
      "0s - loss: 0.0396 - acc: 0.8812 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 394/600\n",
      "0s - loss: 0.0396 - acc: 0.8884 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 395/600\n",
      "0s - loss: 0.0396 - acc: 0.8826 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 396/600\n",
      "0s - loss: 0.0396 - acc: 0.8797 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 397/600\n",
      "0s - loss: 0.0396 - acc: 0.8841 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 398/600\n",
      "0s - loss: 0.0396 - acc: 0.8841 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 399/600\n",
      "0s - loss: 0.0396 - acc: 0.8841 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 400/600\n",
      "0s - loss: 0.0396 - acc: 0.8826 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 401/600\n",
      "0s - loss: 0.0396 - acc: 0.8826 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 402/600\n",
      "0s - loss: 0.0395 - acc: 0.8870 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 403/600\n",
      "0s - loss: 0.0396 - acc: 0.8841 - val_loss: 0.0379 - val_acc: 0.8783\n",
      "Epoch 404/600\n",
      "0s - loss: 0.0396 - acc: 0.8913 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 405/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 406/600\n",
      "0s - loss: 0.0395 - acc: 0.8899 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 407/600\n",
      "0s - loss: 0.0395 - acc: 0.8841 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 408/600\n",
      "0s - loss: 0.0395 - acc: 0.8841 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 409/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 410/600\n",
      "0s - loss: 0.0395 - acc: 0.8841 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 411/600\n",
      "0s - loss: 0.0395 - acc: 0.8870 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 412/600\n",
      "0s - loss: 0.0395 - acc: 0.8855 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 413/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 414/600\n",
      "0s - loss: 0.0395 - acc: 0.8870 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 415/600\n",
      "0s - loss: 0.0395 - acc: 0.8841 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 416/600\n",
      "0s - loss: 0.0395 - acc: 0.8855 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 417/600\n",
      "0s - loss: 0.0395 - acc: 0.8870 - val_loss: 0.0380 - val_acc: 0.8826\n",
      "Epoch 418/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 419/600\n",
      "0s - loss: 0.0395 - acc: 0.8855 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 420/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 421/600\n",
      "0s - loss: 0.0396 - acc: 0.8884 - val_loss: 0.0379 - val_acc: 0.8826\n",
      "Epoch 422/600\n",
      "0s - loss: 0.0395 - acc: 0.8913 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 423/600\n",
      "0s - loss: 0.0394 - acc: 0.8855 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 424/600\n",
      "0s - loss: 0.0394 - acc: 0.8855 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 425/600\n",
      "0s - loss: 0.0395 - acc: 0.8855 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 426/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 427/600\n",
      "0s - loss: 0.0394 - acc: 0.8855 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 428/600\n",
      "0s - loss: 0.0394 - acc: 0.8855 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 429/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 430/600\n",
      "0s - loss: 0.0394 - acc: 0.8899 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 431/600\n",
      "0s - loss: 0.0394 - acc: 0.8884 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 432/600\n",
      "0s - loss: 0.0394 - acc: 0.8855 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 433/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0381 - val_acc: 0.8826\n",
      "Epoch 434/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 435/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 436/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 437/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0380 - val_acc: 0.8826\n",
      "Epoch 438/600\n",
      "0s - loss: 0.0395 - acc: 0.8899 - val_loss: 0.0379 - val_acc: 0.8826\n",
      "Epoch 439/600\n",
      "0s - loss: 0.0395 - acc: 0.8913 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 440/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 441/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 442/600\n",
      "0s - loss: 0.0394 - acc: 0.8884 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 443/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 444/600\n",
      "0s - loss: 0.0394 - acc: 0.8899 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 445/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 446/600\n",
      "0s - loss: 0.0394 - acc: 0.8884 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 447/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 448/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 449/600\n",
      "0s - loss: 0.0394 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 450/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 451/600\n",
      "0s - loss: 0.0395 - acc: 0.8884 - val_loss: 0.0393 - val_acc: 0.8826\n",
      "Epoch 452/600\n",
      "0s - loss: 0.0394 - acc: 0.8870 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 453/600\n",
      "0s - loss: 0.0394 - acc: 0.8913 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 454/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 455/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 456/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 457/600\n",
      "0s - loss: 0.0393 - acc: 0.8899 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 458/600\n",
      "0s - loss: 0.0394 - acc: 0.8884 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 459/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 460/600\n",
      "0s - loss: 0.0393 - acc: 0.8855 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 461/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 462/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 463/600\n",
      "0s - loss: 0.0394 - acc: 0.8884 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 464/600\n",
      "0s - loss: 0.0393 - acc: 0.8855 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 465/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 466/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 467/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 468/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 469/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 470/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0382 - val_acc: 0.8826\n",
      "Epoch 471/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 472/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 473/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 474/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 475/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 476/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 477/600\n",
      "0s - loss: 0.0393 - acc: 0.8899 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 478/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 479/600\n",
      "0s - loss: 0.0393 - acc: 0.8899 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 480/600\n",
      "0s - loss: 0.0394 - acc: 0.8899 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 481/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 482/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 483/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 484/600\n",
      "0s - loss: 0.0393 - acc: 0.8899 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 485/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 486/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 487/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 488/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 489/600\n",
      "0s - loss: 0.0393 - acc: 0.8899 - val_loss: 0.0383 - val_acc: 0.8826\n",
      "Epoch 490/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 491/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 492/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 493/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 494/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 495/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 496/600\n",
      "0s - loss: 0.0392 - acc: 0.8870 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 497/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 498/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 499/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 500/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0384 - val_acc: 0.8826\n",
      "Epoch 501/600\n",
      "0s - loss: 0.0392 - acc: 0.8870 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 502/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 503/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 504/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 505/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 506/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 507/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 508/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 509/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 510/600\n",
      "0s - loss: 0.0392 - acc: 0.8870 - val_loss: 0.0393 - val_acc: 0.8826\n",
      "Epoch 511/600\n",
      "0s - loss: 0.0393 - acc: 0.8899 - val_loss: 0.0394 - val_acc: 0.8870\n",
      "Epoch 512/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0396 - val_acc: 0.8870\n",
      "Epoch 513/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 514/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 515/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 516/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 517/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 518/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 519/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 520/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0393 - val_acc: 0.8826\n",
      "Epoch 521/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0395 - val_acc: 0.8870\n",
      "Epoch 522/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 523/600\n",
      "0s - loss: 0.0392 - acc: 0.8870 - val_loss: 0.0393 - val_acc: 0.8826\n",
      "Epoch 524/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 525/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 526/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 527/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0395 - val_acc: 0.8870\n",
      "Epoch 528/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0397 - val_acc: 0.8913\n",
      "Epoch 529/600\n",
      "0s - loss: 0.0393 - acc: 0.8884 - val_loss: 0.0394 - val_acc: 0.8870\n",
      "Epoch 530/600\n",
      "0s - loss: 0.0392 - acc: 0.8913 - val_loss: 0.0396 - val_acc: 0.8870\n",
      "Epoch 531/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 532/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 533/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 534/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 535/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 536/600\n",
      "0s - loss: 0.0393 - acc: 0.8870 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 537/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 538/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 539/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 540/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 541/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 542/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 543/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 544/600\n",
      "0s - loss: 0.0392 - acc: 0.8870 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 545/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 546/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 547/600\n",
      "0s - loss: 0.0392 - acc: 0.8870 - val_loss: 0.0394 - val_acc: 0.8826\n",
      "Epoch 548/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0395 - val_acc: 0.8870\n",
      "Epoch 549/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0396 - val_acc: 0.8870\n",
      "Epoch 550/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 551/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 552/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 553/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0386 - val_acc: 0.8826\n",
      "Epoch 554/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 555/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 556/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 557/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 558/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 559/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 560/600\n",
      "0s - loss: 0.0392 - acc: 0.8870 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 561/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 562/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 563/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 564/600\n",
      "0s - loss: 0.0391 - acc: 0.8870 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 565/600\n",
      "0s - loss: 0.0392 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 566/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 567/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 568/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 569/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 570/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 571/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 572/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 573/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0387 - val_acc: 0.8826\n",
      "Epoch 574/600\n",
      "0s - loss: 0.0392 - acc: 0.8899 - val_loss: 0.0385 - val_acc: 0.8826\n",
      "Epoch 575/600\n",
      "0s - loss: 0.0391 - acc: 0.8913 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 576/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0388 - val_acc: 0.8826\n",
      "Epoch 577/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 578/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 579/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 580/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 581/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 582/600\n",
      "0s - loss: 0.0391 - acc: 0.8870 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 583/600\n",
      "0s - loss: 0.0391 - acc: 0.8870 - val_loss: 0.0393 - val_acc: 0.8826\n",
      "Epoch 584/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0394 - val_acc: 0.8826\n",
      "Epoch 585/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0396 - val_acc: 0.8870\n",
      "Epoch 586/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0395 - val_acc: 0.8870\n",
      "Epoch 587/600\n",
      "0s - loss: 0.0391 - acc: 0.8913 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 588/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0394 - val_acc: 0.8826\n",
      "Epoch 589/600\n",
      "0s - loss: 0.0391 - acc: 0.8913 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 590/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0393 - val_acc: 0.8826\n",
      "Epoch 591/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 592/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 593/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0389 - val_acc: 0.8826\n",
      "Epoch 594/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Epoch 595/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 596/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0393 - val_acc: 0.8826\n",
      "Epoch 597/600\n",
      "0s - loss: 0.0391 - acc: 0.8899 - val_loss: 0.0395 - val_acc: 0.8826\n",
      "Epoch 598/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0392 - val_acc: 0.8826\n",
      "Epoch 599/600\n",
      "0s - loss: 0.0391 - acc: 0.8870 - val_loss: 0.0390 - val_acc: 0.8826\n",
      "Epoch 600/600\n",
      "0s - loss: 0.0391 - acc: 0.8884 - val_loss: 0.0391 - val_acc: 0.8826\n",
      "Train Score:  96.0961212412\n",
      "Test Score:  96.0892192955\n"
     ]
    }
   ],
   "source": [
    "m2=make_model('sigmoid','rmsprop',tx_train.shape[1],[tx_train.shape[1]*10,16],tx_train,ty_train,tx_test,ty_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision model 2: 93\n"
     ]
    }
   ],
   "source": [
    "pr=m2.predict(tx_test)\n",
    "#print(pr)\n",
    "u=0\n",
    "k=0\n",
    "tp=0\n",
    "p=0\n",
    "for u in range(len(tx_test)):\n",
    "    if round(pr[u][0],1)>=0.2 and round(pr[u][0],1)<=0.75:\n",
    "        g=tsvmpred[u]\n",
    "    else:\n",
    "        g=round(pr[u][0],0)\n",
    "    if g!=ty_test[u]:    \n",
    "        #print \"expected\",ty_test[u],\"predicted:\",pr[u][0],\" \",tsvmpred[u]\n",
    "        k=k+1\n",
    "    if g==1:\n",
    "        p=p+1\n",
    "        if y_test[u]==1:\n",
    "            tp=tp+1\n",
    "#print \"error\",k*100/len(y_test),\"%\"\n",
    "print \"precision model 2:\",tp*100/p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision for various classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative 85\n",
      "False Positive 11\n",
      "False Negative 13\n",
      "True Positive 121\n",
      "precision svm: 91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,clfsvm.predict(x_test)).ravel()\n",
    "print \"True Negative\",tn\n",
    "print \"False Positive\",fp\n",
    "print \"False Negative\",fn\n",
    "print \"True Positive\",tp\n",
    "\n",
    "\n",
    "print \"precision svm:\",tp*100/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative 83\n",
      "False Positive 13\n",
      "False Negative 13\n",
      "True Positive 121\n",
      "precision logistics: 90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,lrcv.predict(x_test)).ravel()\n",
    "print \"True Negative\",tn\n",
    "print \"False Positive\",fp\n",
    "print \"False Negative\",fn\n",
    "print \"True Positive\",tp\n",
    "\n",
    "\n",
    "print \"precision logistics:\",tp*100/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative 87\n",
      "False Positive 9\n",
      "False Negative 14\n",
      "True Positive 120\n",
      "precision t_svm: 93\n",
      "\n",
      "\n",
      "True Negative 86\n",
      "False Positive 10\n",
      "False Negative 13\n",
      "True Positive 121\n",
      "precision t_lrcv: 92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix#svm with selected features\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,tclf.predict(tx_test)).ravel()\n",
    "print \"True Negative\",tn\n",
    "print \"False Positive\",fp\n",
    "print \"False Negative\",fn\n",
    "print \"True Positive\",tp\n",
    "\n",
    "\n",
    "print \"precision t_svm:\",tp*100/(tp+fp)\n",
    "\n",
    "print \n",
    "print\n",
    "\n",
    "from sklearn.metrics import confusion_matrix#logistics with selected features\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,tlrcv.predict(tx_test)).ravel()\n",
    "print \"True Negative\",tn\n",
    "print \"False Positive\",fp\n",
    "print \"False Negative\",fn\n",
    "print \"True Positive\",tp\n",
    "\n",
    "\n",
    "print \"precision t_lrcv:\",tp*100/(tp+fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision with important feature selected SVM is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87391304347826082"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf\n",
    "\n",
    "clftree = ExtraTreesClassifier()\n",
    "clftree.fit(x_train,y_train)\n",
    "clftree.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying ensemble techniques to get some improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87065217391304339"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "num_trees = 100\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model,dat,labels, cv=kfold)\n",
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85760869565217401"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "seed = 42\n",
    "num_trees =100\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model,dat,labels, cv=kfold)\n",
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88152173913\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "seed = 42\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "cart = svm.SVC(gamma=0.001, C=100)\n",
    "num_trees = 64\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model,dat,labels, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with selected features has the best precision of 93%.(accuracy 90%)\n",
    "### Neural n/w with the sigmoid activation and important features has a precision of 93%(accuracy of ~96.%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo\n",
    "\n",
    "#pickel the model and complete the UI to take input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
